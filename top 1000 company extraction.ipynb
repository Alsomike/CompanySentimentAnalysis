{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d937c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google\n",
      "  Downloading google-3.0.0-py2.py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from google) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from beautifulsoup4->google) (2.3.1)\n",
      "Installing collected packages: google\n",
      "Successfully installed google-3.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d009e44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Using cached yfinance-0.2.20-py2.py3-none-any.whl (62 kB)\n",
      "Collecting html5lib>=1.1\n",
      "  Using cached html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from yfinance) (4.9.2)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from yfinance) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from yfinance) (1.21.5)\n",
      "Requirement already satisfied: cryptography>=3.3.2 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from yfinance) (3.4.8)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from yfinance) (1.4.2)\n",
      "Collecting frozendict>=2.3.4\n",
      "  Using cached frozendict-2.3.8-cp39-cp39-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from yfinance) (4.11.1)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: requests>=2.26 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from yfinance) (2.27.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.3.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from cryptography>=3.3.2->yfinance) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.21)\n",
      "Requirement already satisfied: webencodings in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from requests>=2.26->yfinance) (2.0.4)\n",
      "Installing collected packages: html5lib, frozendict, yfinance\n",
      "Successfully installed frozendict-2.3.8 html5lib-1.1 yfinance-0.2.20\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b6c0e40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search as google_search\n",
    "import yfinance as yf\n",
    "\n",
    "def get_article_keywords(url):\n",
    "    # Define the keywords related to good news and bad news\n",
    "    good_news_keywords = ['positive', 'improvement', 'growth', 'increase']\n",
    "    bad_news_keywords = ['negative', 'decline', 'loss', 'decrease']\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the article URL\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content of the article\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get the text content of the article\n",
    "        article_text = soup.get_text().lower()\n",
    "\n",
    "        # Count the occurrences of good news and bad news keywords\n",
    "        good_news_count = sum(article_text.count(keyword) for keyword in good_news_keywords)\n",
    "        bad_news_count = sum(article_text.count(keyword) for keyword in bad_news_keywords)\n",
    "\n",
    "        return good_news_count, bad_news_count\n",
    "\n",
    "    except (requests.RequestException, ValueError, AttributeError) as e:\n",
    "        print(f\"Error accessing article: {url}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_articles(company_name):\n",
    "    # Perform a Google search for the company name\n",
    "    search_results = google_search(company_name, num=100, stop=100, pause=2.0)\n",
    "\n",
    "    # Iterate through the search results and process the articles\n",
    "    for url in search_results:\n",
    "        # Check if the URL is an article link\n",
    "        if '/article/' in url:\n",
    "            keyword_counts = get_article_keywords(url)\n",
    "\n",
    "            if keyword_counts is None:\n",
    "                continue\n",
    "\n",
    "            good_news_count, bad_news_count = keyword_counts\n",
    "\n",
    "            # Print the results for each article\n",
    "            print(f\"Article URL: {url}\")\n",
    "            print(f\"Good News Count: {good_news_count}\")\n",
    "            print(f\"Bad News Count: {bad_news_count}\")\n",
    "            print('---')\n",
    "\n",
    "def get_top_1000_companies():\n",
    "    url = \"https://stockanalysis.com/list/biggest-companies/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\", {\"class\": \"table table-striped\"})\n",
    "    rows = table.find_all(\"tr\")\n",
    "\n",
    "    symbols = []\n",
    "\n",
    "    for row in rows[1:]:\n",
    "        symbol = row.find_all(\"td\")[0].text.strip()\n",
    "        symbols.append(symbol)\n",
    "\n",
    "    return symbols[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ae9d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Container element not found on the webpage.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search as google_search\n",
    "import yfinance as yf\n",
    "\n",
    "def get_article_keywords(url):\n",
    "    # Define the keywords related to good news and bad news\n",
    "    good_news_keywords = ['positive', 'improvement', 'growth', 'increase']\n",
    "    bad_news_keywords = ['negative', 'decline', 'loss', 'decrease']\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the article URL\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content of the article\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get the text content of the article\n",
    "        article_text = soup.get_text().lower()\n",
    "\n",
    "        # Count the occurrences of good news and bad news keywords\n",
    "        good_news_count = sum(article_text.count(keyword) for keyword in good_news_keywords)\n",
    "        bad_news_count = sum(article_text.count(keyword) for keyword in bad_news_keywords)\n",
    "\n",
    "        return good_news_count, bad_news_count\n",
    "\n",
    "    except (requests.RequestException, ValueError, AttributeError) as e:\n",
    "        print(f\"Error accessing article: {url}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_articles(company_name):\n",
    "    # Perform a Google search for the company name\n",
    "    search_results = google_search(company_name, num=100, stop=100, pause=2.0)\n",
    "\n",
    "    # Iterate through the search results and process the articles\n",
    "    for url in search_results:\n",
    "        # Check if the URL is an article link\n",
    "        if '/article/' in url:\n",
    "            keyword_counts = get_article_keywords(url)\n",
    "\n",
    "            if keyword_counts is None:\n",
    "                continue\n",
    "\n",
    "            good_news_count, bad_news_count = keyword_counts\n",
    "\n",
    "            # Print the results for each article\n",
    "            print(f\"Article URL: {url}\")\n",
    "            print(f\"Good News Count: {good_news_count}\")\n",
    "            print(f\"Bad News Count: {bad_news_count}\")\n",
    "            print('---')\n",
    "\n",
    "def get_top_1000_companies():\n",
    "    url = \"https://stockanalysis.com/list/biggest-companies/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    symbols = []\n",
    "\n",
    "    # Find the container element that holds the company symbols\n",
    "    container = soup.find(\"ul\", {\"class\": \"table-list\"})\n",
    "    if container is None:\n",
    "        print(\"Error: Container element not found on the webpage.\")\n",
    "        return []\n",
    "\n",
    "    # Find all the list items within the container\n",
    "    list_items = container.find_all(\"li\")\n",
    "    for item in list_items:\n",
    "        # Find the symbol within each list item\n",
    "        symbol = item.find(\"span\", {\"class\": \"symbol\"}).text.strip()\n",
    "        symbols.append(symbol)\n",
    "\n",
    "    return symbols[:1000]\n",
    "\n",
    "# Get the list of top 1000 companies\n",
    "company_list = get_top_1000_companies()\n",
    "\n",
    "# Iterate through the company list and scrape articles for each company\n",
    "for company in company_list:\n",
    "    scrape_articles(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "232d0393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Container element not found on the webpage.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_company_names():\n",
    "    url = \"https://stockanalysis.com/list/biggest-companies/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    names = []\n",
    "\n",
    "    # Find the container element that holds the company names\n",
    "    container = soup.find(\"ul\", {\"class\": \"table-list\"})\n",
    "    if container is None:\n",
    "        print(\"Error: Container element not found on the webpage.\")\n",
    "        return []\n",
    "\n",
    "    # Find all the list items within the container\n",
    "    list_items = container.find_all(\"li\")\n",
    "    for item in list_items:\n",
    "        # Find the company name within each list item\n",
    "        name = item.find(\"span\", {\"class\": \"company\"}).text.strip()\n",
    "        names.append(name)\n",
    "\n",
    "    return names[:1000]\n",
    "\n",
    "# Get the list of company names\n",
    "company_names = get_company_names()\n",
    "\n",
    "# Print the company names\n",
    "for name in company_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bd0ccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc.\n",
      "Microsoft Corporation\n",
      "Alphabet Inc.\n",
      "Amazon.com, Inc.\n",
      "NVIDIA Corporation\n",
      "Tesla, Inc.\n",
      "Berkshire Hathaway Inc.\n",
      "Meta Platforms, Inc.\n",
      "Taiwan Semiconductor Manufacturing Company Limited\n",
      "Visa Inc.\n",
      "UnitedHealth Group Incorporated\n",
      "Exxon Mobil Corporation\n",
      "Eli Lilly and Company\n",
      "Walmart Inc.\n",
      "Johnson & Johnson\n",
      "JPMorgan Chase & Co.\n",
      "Broadcom Inc.\n",
      "Novo Nordisk A/S\n",
      "Mastercard Incorporated\n",
      "The Procter & Gamble Company\n",
      "Oracle Corporation\n",
      "The Home Depot, Inc.\n",
      "Chevron Corporation\n",
      "ASML Holding N.V.\n",
      "Merck & Co., Inc.\n",
      "The Coca-Cola Company\n",
      "PepsiCo, Inc.\n",
      "AbbVie Inc.\n",
      "Costco Wholesale Corporation\n",
      "Bank of America Corporation\n",
      "Alibaba Group Holding Limited\n",
      "BHP Group Limited\n",
      "AstraZeneca PLC\n",
      "Toyota Motor Corporation\n",
      "Pfizer Inc.\n",
      "Adobe Inc.\n",
      "Novartis AG\n",
      "McDonald's Corporation\n",
      "Cisco Systems, Inc.\n",
      "Advanced Micro Devices, Inc.\n",
      "Salesforce, Inc.\n",
      "Shell plc\n",
      "Thermo Fisher Scientific Inc.\n",
      "Accenture plc\n",
      "Fomento Económico Mexicano, SAB de CV\n",
      "Netflix, Inc.\n",
      "Linde plc\n",
      "Abbott Laboratories\n",
      "Danaher Corporation\n",
      "NIKE, Inc.\n",
      "Comcast Corporation\n",
      "The Walt Disney Company\n",
      "Texas Instruments Incorporated\n",
      "SAP SE\n",
      "Wells Fargo & Company\n",
      "HSBC Holdings plc\n",
      "T-Mobile US, Inc.\n",
      "Verizon Communications Inc.\n",
      "United Parcel Service, Inc.\n",
      "NextEra Energy, Inc.\n",
      "Intel Corporation\n",
      "Morgan Stanley\n",
      "Philip Morris International Inc.\n",
      "Raytheon Technologies Corporation\n",
      "TotalEnergies SE\n",
      "QUALCOMM Incorporated\n",
      "Bristol-Myers Squibb Company\n",
      "Honeywell International Inc.\n",
      "The Boeing Company\n",
      "Royal Bank of Canada\n",
      "American Express Company\n",
      "Sanofi\n",
      "Unilever PLC\n",
      "S&P Global Inc.\n",
      "Lowe's Companies, Inc.\n",
      "Caterpillar Inc.\n",
      "Intuit Inc.\n",
      "International Business Machines Corporation\n",
      "ConocoPhillips\n",
      "Union Pacific Corporation\n",
      "Sony Group Corporation\n",
      "HDFC Bank Limited\n",
      "Amgen Inc.\n",
      "Applied Materials, Inc.\n",
      "Deere & Company\n",
      "Medtronic plc\n",
      "ServiceNow, Inc.\n",
      "Starbucks Corporation\n",
      "Lockheed Martin Corporation\n",
      "General Electric Company\n",
      "AT&T Inc.\n",
      "Intuitive Surgical, Inc.\n",
      "Prologis, Inc.\n",
      "The Goldman Sachs Group, Inc.\n",
      "Anheuser-Busch InBev SA/NV\n",
      "Stryker Corporation\n",
      "The Toronto-Dominion Bank\n",
      "Rio Tinto Group\n",
      "Blackstone Inc.\n",
      "PDD Holdings Inc.\n",
      "BlackRock, Inc.\n",
      "Elevance Health Inc.\n",
      "BP p.l.c.\n",
      "Mondelez International, Inc.\n",
      "Gilead Sciences, Inc.\n",
      "Diageo plc\n",
      "Booking Holdings Inc.\n",
      "Analog Devices, Inc.\n",
      "The Charles Schwab Corporation\n",
      "Citigroup Inc.\n",
      "Equinor ASA\n",
      "The TJX Companies, Inc.\n",
      "Petróleo Brasileiro S.A. - Petrobras\n",
      "American Tower Corporation\n",
      "Automatic Data Processing, Inc.\n",
      "Marsh & McLennan Companies, Inc.\n",
      "Vertex Pharmaceuticals Incorporated\n",
      "CVS Health Corporation\n",
      "Lam Research Corporation\n",
      "Shopify Inc.\n",
      "Mitsubishi UFJ Financial Group, Inc.\n",
      "Uber Technologies, Inc.\n",
      "Regeneron Pharmaceuticals, Inc.\n",
      "Petróleo Brasileiro S.A. - Petrobras\n",
      "ICICI Bank Limited\n",
      "Airbnb, Inc.\n",
      "Altria Group, Inc.\n",
      "Chubb Limited\n",
      "HCA Healthcare, Inc.\n",
      "Cigna Corporation\n",
      "Canadian National Railway Company\n",
      "Eaton Corporation plc\n",
      "The Southern Company\n",
      "Boston Scientific Corporation\n",
      "Zoetis Inc.\n",
      "Micron Technology, Inc.\n",
      "Enbridge Inc.\n",
      "Illinois Tool Works Inc.\n",
      "The Progressive Corporation\n",
      "Palo Alto Networks, Inc.\n",
      "Equinix, Inc.\n",
      "Canadian Pacific Railway Limited\n",
      "British American Tobacco p.l.c.\n",
      "Becton, Dickinson and Company\n",
      "PayPal Holdings, Inc.\n",
      "América Móvil, SAB de CV\n",
      "GSK plc\n",
      "Fiserv, Inc.\n",
      "Duke Energy Corporation\n",
      "The Estée Lauder Companies Inc.\n",
      "Northrop Grumman Corporation\n",
      "Synopsys, Inc.\n",
      "Schlumberger Limited\n",
      "CSX Corporation\n",
      "Waste Management, Inc.\n",
      "Aon plc\n",
      "KLA Corporation\n",
      "KKR & Co. Inc.\n",
      "CME Group Inc.\n",
      "EOG Resources, Inc.\n",
      "Infosys Limited\n",
      "Cadence Design Systems, Inc.\n",
      "Air Products and Chemicals, Inc.\n",
      "Activision Blizzard, Inc.\n",
      "UBS Group AG\n",
      "NetEase, Inc.\n",
      "The Sherwin-Williams Company\n",
      "Bank of Montreal\n",
      "Colgate-Palmolive Company\n",
      "Moody's Corporation\n",
      "Vale S.A.\n",
      "Intercontinental Exchange, Inc.\n",
      "RELX PLC\n",
      "Target Corporation\n",
      "Monster Beverage Corporation\n",
      "MercadoLibre, Inc.\n",
      "VMware, Inc.\n",
      "Canadian Natural Resources Limited\n",
      "JD.com, Inc.\n",
      "Thomson Reuters Corporation\n",
      "The Bank of Nova Scotia\n",
      "Snowflake Inc.\n",
      "General Dynamics Corporation\n",
      "FedEx Corporation\n",
      "Freeport-McMoRan Inc.\n",
      "Southern Copper Corporation\n",
      "Chipotle Mexican Grill, Inc.\n",
      "Humana Inc.\n",
      "Itaú Unibanco Holding S.A.\n",
      "Banco Santander, S.A.\n",
      "Workday, Inc.\n",
      "Enterprise Products Partners L.P.\n",
      "3M Company\n",
      "Fortinet, Inc.\n",
      "Ford Motor Company\n",
      "Sumitomo Mitsui Financial Group, Inc.\n",
      "O'Reilly Automotive, Inc.\n",
      "Marriott International, Inc.\n",
      "Ferrari N.V.\n",
      "Edwards Lifesciences Corporation\n",
      "Marvell Technology, Inc.\n",
      "Honda Motor Co., Ltd.\n",
      "The Hershey Company\n",
      "Stellantis N.V.\n",
      "McKesson Corporation\n",
      "Occidental Petroleum Corporation\n",
      "General Motors Company\n",
      "Brookfield Corporation\n",
      "Arista Networks, Inc.\n",
      "Baidu, Inc.\n",
      "Ecolab Inc.\n",
      "NXP Semiconductors N.V.\n",
      "The PNC Financial Services Group, Inc.\n",
      "Charter Communications, Inc.\n",
      "Norfolk Southern Corporation\n",
      "Takeda Pharmaceutical Company Limited\n",
      "Public Storage\n",
      "Crown Castle Inc.\n",
      "Ambev S.A.\n",
      "Cintas Corporation\n",
      "DexCom, Inc.\n",
      "U.S. Bancorp\n",
      "Emerson Electric Co.\n",
      "Roper Technologies, Inc.\n",
      "National Grid plc\n",
      "Kenvue Inc.\n",
      "Amphenol Corporation\n",
      "Moderna, Inc.\n",
      "Marathon Petroleum Corporation\n",
      "Lululemon Athletica Inc.\n",
      "ING Groep N.V.\n",
      "General Mills, Inc.\n",
      "Microchip Technology Incorporated\n",
      "Parker-Hannifin Corporation\n",
      "Pioneer Natural Resources Company\n",
      "Eni S.p.A.\n",
      "Motorola Solutions, Inc.\n",
      "Atlassian Corporation Plc\n",
      "Sempra\n",
      "Banco Santander (Brasil) S.A.\n",
      "Las Vegas Sands Corp.\n",
      "Autodesk, Inc.\n",
      "Kimberly-Clark Corporation\n",
      "Republic Services, Inc.\n",
      "Constellation Brands, Inc.\n",
      "STMicroelectronics N.V.\n",
      "The Kraft Heinz Company\n",
      "Arthur J. Gallagher & Co.\n",
      "Apollo Global Management, Inc.\n",
      "Keurig Dr Pepper Inc.\n",
      "TransDigm Group Incorporated\n",
      "Woodside Energy Group Ltd\n",
      "Johnson Controls International plc\n",
      "Dominion Energy, Inc.\n",
      "Phillips 66\n",
      "AutoZone, Inc.\n",
      "Banco Bilbao Vizcaya Argentaria, S.A.\n",
      "Biogen Inc.\n",
      "American Electric Power Company, Inc.\n",
      "Capital One Financial Corporation\n",
      "PG&E Corporation\n",
      "Truist Financial Corporation\n",
      "Aflac Incorporated\n",
      "TE Connectivity Ltd.\n",
      "Simon Property Group, Inc.\n",
      "Trane Technologies plc\n",
      "MetLife, Inc.\n",
      "BCE Inc.\n",
      "Copart, Inc.\n",
      "Realty Income Corporation\n",
      "Hess Corporation\n",
      "TC Energy Corporation\n",
      "PACCAR Inc\n",
      "Corteva, Inc.\n",
      "Welltower Inc.\n",
      "Valero Energy Corporation\n",
      "American International Group, Inc.\n",
      "Paychex, Inc.\n",
      "The Travelers Companies, Inc.\n",
      "ON Semiconductor Corporation\n",
      "Exelon Corporation\n",
      "Archer-Daniels-Midland Company\n",
      "Energy Transfer LP\n",
      "Canadian Imperial Bank of Commerce\n",
      "Prudential plc\n",
      "IQVIA Holdings Inc.\n",
      "Alcon Inc.\n",
      "Suncor Energy Inc.\n",
      "MSCI Inc.\n",
      "D.R. Horton, Inc.\n",
      "IDEXX Laboratories, Inc.\n",
      "Carrier Global Corporation\n",
      "Block, Inc.\n",
      "Mizuho Financial Group, Inc.\n",
      "Yum! Brands, Inc.\n",
      "CRH plc\n",
      "Kinder Morgan, Inc.\n",
      "Hilton Worldwide Holdings Inc.\n",
      "Haleon plc\n",
      "Nucor Corporation\n",
      "The Trade Desk, Inc.\n",
      "Dow Inc.\n",
      "Banco Bradesco S.A.\n",
      "Sysco Corporation\n",
      "Lloyds Banking Group plc\n",
      "Seagen Inc.\n",
      "The Williams Companies, Inc.\n",
      "CrowdStrike Holdings, Inc.\n",
      "W.W. Grainger, Inc.\n",
      "AmerisourceBergen Corporation\n",
      "Sea Limited\n",
      "Ross Stores, Inc.\n",
      "L3Harris Technologies, Inc.\n",
      "Otis Worldwide Corporation\n",
      "Rockwell Automation, Inc.\n",
      "Dell Technologies Inc.\n",
      "Cheniere Energy, Inc.\n",
      "GE HealthCare Technologies Inc.\n",
      "Dollar General Corporation\n",
      "Centene Corporation\n",
      "AMETEK, Inc.\n",
      "Manulife Financial Corporation\n",
      "Agilent Technologies, Inc.\n",
      "Waste Connections, Inc.\n",
      "Old Dominion Freight Line, Inc.\n",
      "Electronic Arts Inc.\n",
      "The Bank of New York Mellon Corporation\n",
      "Restaurant Brands International Inc.\n",
      "Nu Holdings Ltd.\n",
      "GLOBALFOUNDRIES Inc.\n",
      "Xcel Energy Inc.\n",
      "The Kroger Co.\n",
      "CoStar Group, Inc.\n",
      "MPLX LP\n",
      "Palantir Technologies Inc.\n",
      "Ameriprise Financial, Inc.\n",
      "Newmont Corporation\n",
      "Warner Bros. Discovery, Inc.\n",
      "Interactive Brokers Group, Inc.\n",
      "Cummins Inc.\n",
      "PPG Industries, Inc.\n",
      "Li Auto Inc.\n",
      "Mobileye Global Inc.\n",
      "Lennar Corporation\n",
      "VICI Properties Inc.\n",
      "Fidelity National Information Services, Inc.\n",
      "Illumina, Inc.\n",
      "Cognizant Technology Solutions Corporation\n",
      "DuPont de Nemours, Inc.\n",
      "Consolidated Edison, Inc.\n",
      "Verisk Analytics, Inc.\n",
      "ResMed Inc.\n",
      "Chunghwa Telecom Co., Ltd.\n",
      "Cenovus Energy Inc.\n",
      "Brown-Forman Corporation\n",
      "Fastenal Company\n",
      "Devon Energy Corporation\n",
      "Prudential Financial, Inc.\n",
      "Veeva Systems Inc.\n",
      "Datadog, Inc.\n",
      "Barclays PLC\n",
      "Public Service Enterprise Group Incorporated\n",
      "Digital Realty Trust, Inc.\n",
      "Ferguson plc\n",
      "HP Inc.\n",
      "Dollar Tree, Inc.\n",
      "Baker Hughes Company\n",
      "Constellation Energy Corporation\n",
      "NatWest Group plc\n",
      "Coca-Cola Europacific Partners PLC\n",
      "Coupang, Inc.\n",
      "Halliburton Company\n",
      "Zimmer Biomet Holdings, Inc.\n",
      "Discover Financial Services\n",
      "Orange S.A.\n",
      "Sun Life Financial Inc.\n",
      "Barrick Gold Corporation\n",
      "LyondellBasell Industries N.V.\n",
      "Spotify Technology S.A.\n",
      "Keysight Technologies, Inc.\n",
      "The Allstate Corporation\n",
      "Imperial Oil Limited\n",
      "Nutrien Ltd.\n",
      "ANSYS, Inc.\n",
      "Mettler-Toledo International Inc.\n",
      "WEC Energy Group, Inc.\n",
      "Ares Management Corporation\n",
      "American Water Works Company, Inc.\n",
      "Corning Incorporated\n",
      "Equifax Inc.\n",
      "United Rentals, Inc.\n",
      "Gartner, Inc.\n",
      "DoorDash, Inc.\n",
      "Franco-Nevada Corporation\n",
      "TELUS Corporation\n",
      "Vulcan Materials Company\n",
      "Aptiv PLC\n",
      "Walgreens Boots Alliance, Inc.\n",
      "Delta Air Lines, Inc.\n",
      "ONEOK, Inc.\n",
      "AvalonBay Communities, Inc.\n",
      "Quanta Services, Inc.\n",
      "Xylem Inc.\n",
      "Symbotic Inc.\n",
      "BioNTech SE\n",
      "Albemarle Corporation\n",
      "Perusahaan Perseroan (Persero) PT Telekomunikasi Indonesia Tbk\n",
      "Global Payments Inc.\n",
      "MongoDB, Inc.\n",
      "Martin Marietta Materials, Inc.\n",
      "West Pharmaceutical Services, Inc.\n",
      "Edison International\n",
      "Arch Capital Group Ltd.\n",
      "Ingersoll Rand Inc.\n",
      "Wipro Limited\n",
      "Monolithic Power Systems, Inc.\n",
      "Equity Residential\n",
      "Nasdaq, Inc.\n",
      "Genmab A/S\n",
      "HubSpot, Inc.\n",
      "T. Rowe Price Group, Inc.\n",
      "Vodafone Group PLC\n",
      "SBA Communications Corporation\n",
      "Agnico Eagle Mines Limited\n",
      "Alnylam Pharmaceuticals, Inc.\n",
      "Yum China Holdings, Inc.\n",
      "Align Technology, Inc.\n",
      "CGI Inc.\n",
      "Enphase Energy, Inc.\n",
      "State Street Corporation\n",
      "Fortive Corporation\n",
      "Ryanair Holdings plc\n",
      "McCormick & Company, Incorporated\n",
      "Eversource Energy\n",
      "Roblox Corporation\n",
      "CBRE Group, Inc.\n",
      "Trip.com Group Limited\n",
      "Willis Towers Watson PLC\n",
      "Royal Caribbean Cruises Ltd.\n",
      "eBay Inc.\n",
      "Tractor Supply Company\n",
      "CDW Corporation\n",
      "UDR, Inc.\n",
      "ArcelorMittal S.A.\n",
      "POSCO Holdings Inc.\n",
      "Rogers Communications Inc.\n",
      "Church & Dwight Co., Inc.\n",
      "DTE Energy Company\n",
      "Take-Two Interactive Software, Inc.\n",
      "Nokia Oyj\n",
      "Horizon Therapeutics PLC\n",
      "Zscaler, Inc.\n",
      "VeriSign, Inc.\n",
      "Diamondback Energy, Inc.\n",
      "Cloudflare, Inc.\n",
      "Kellogg Company\n",
      "ZTO Express (Cayman) Inc.\n",
      "Telefónica, S.A.\n",
      "Hormel Foods Corporation\n",
      "Weyerhaeuser Company\n",
      "Genuine Parts Company\n",
      "FirstEnergy Corp.\n",
      "United Microelectronics Corporation\n",
      "Cardinal Health, Inc.\n",
      "Ulta Beauty, Inc.\n",
      "Companhia Paranaense de Energia - COPEL\n",
      "Teck Resources Limited\n",
      "Baxter International Inc.\n",
      "Cheniere Energy Partners, L.P.\n",
      "The Hartford Financial Services Group, Inc.\n",
      "argenx SE\n",
      "BeiGene, Ltd.\n",
      "Deutsche Bank Aktiengesellschaft\n",
      "Ameren Corporation\n",
      "Hewlett Packard Enterprise Company\n",
      "KE Holdings Inc.\n",
      "Entergy Corporation\n",
      "Alexandria Real Estate Equities, Inc.\n",
      "Ecopetrol S.A.\n",
      "STERIS plc\n",
      "Invitation Homes Inc.\n",
      "Raymond James Financial, Inc.\n",
      "First Solar, Inc.\n",
      "Fortis Inc.\n",
      "Rentokil Initial plc\n",
      "Extra Space Storage Inc.\n",
      "Garmin Ltd.\n",
      "Zoom Video Communications, Inc.\n",
      "Meta Data Limited\n",
      "Carnival Corporation & plc\n",
      "Dover Corporation\n",
      "Darden Restaurants, Inc.\n",
      "Sociedad Química y Minera de Chile S.A.\n",
      "M&T Bank Corporation\n",
      "Wheaton Precious Metals Corp.\n",
      "Rollins, Inc.\n",
      "Laboratory Corporation of America Holdings\n",
      "ORIX Corporation\n",
      "Live Nation Entertainment, Inc.\n",
      "Southwest Airlines Co.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_company_names():\n",
    "    url = \"https://stockanalysis.com/list/biggest-companies/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    names = []\n",
    "\n",
    "    # Find all the table cells with the specified class\n",
    "    cells = soup.find_all(\"td\", class_=\"slw svelte-1tv1ofl\")\n",
    "    for cell in cells:\n",
    "        # Extract the text content of each cell (company name)\n",
    "        name = cell.text.strip()\n",
    "        names.append(name)\n",
    "\n",
    "    return names[:1000]\n",
    "\n",
    "# Get the list of company names\n",
    "company_names = get_company_names()\n",
    "\n",
    "# Print the company names\n",
    "for name in company_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "154d03ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 429: Too Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 80>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Iterate through the company list and scrape articles for each company\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m company \u001b[38;5;129;01min\u001b[39;00m company_list:\n\u001b[1;32m---> 81\u001b[0m     \u001b[43mscrape_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36mscrape_articles\u001b[1;34m(company_name)\u001b[0m\n\u001b[0;32m     36\u001b[0m search_results \u001b[38;5;241m=\u001b[39m google_search(company_name, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, pause\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Iterate through the search results and process the articles\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m search_results:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Check if the URL is an article link\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/article/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m url:\n\u001b[0;32m     42\u001b[0m         keyword_counts \u001b[38;5;241m=\u001b[39m get_article_keywords(url)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:305\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(query, tld, lang, tbs, safe, num, start, stop, pause, country, extra_params, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    302\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(pause)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# Request the Google Search results page.\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mget_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_ssl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Parse the response and get every anchored URL.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bs4:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:174\u001b[0m, in \u001b[0;36mget_page\u001b[1;34m(url, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    172\u001b[0m cookie_jar\u001b[38;5;241m.\u001b[39madd_cookie_header(request)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_ssl:\n\u001b[1;32m--> 174\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     context \u001b[38;5;241m=\u001b[39m ssl\u001b[38;5;241m.\u001b[39m_create_unverified_context()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:555\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    553\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    554\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[1;32m--> 555\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:747\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    744\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    745\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 429: Too Many Requests"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search as google_search\n",
    "import yfinance as yf\n",
    "\n",
    "def get_article_keywords(url):\n",
    "    # Define the keywords related to good news and bad news\n",
    "    good_news_keywords = ['positive', 'improvement', 'growth', 'increase']\n",
    "    bad_news_keywords = ['negative', 'decline', 'loss', 'decrease']\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the article URL\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content of the article\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get the text content of the article\n",
    "        article_text = soup.get_text().lower()\n",
    "\n",
    "        # Count the occurrences of good news and bad news keywords\n",
    "        good_news_count = sum(article_text.count(keyword) for keyword in good_news_keywords)\n",
    "        bad_news_count = sum(article_text.count(keyword) for keyword in bad_news_keywords)\n",
    "\n",
    "        return good_news_count, bad_news_count\n",
    "\n",
    "    except (requests.RequestException, ValueError, AttributeError) as e:\n",
    "        print(f\"Error accessing article: {url}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_articles(company_name):\n",
    "    # Perform a Google search for the company name\n",
    "    search_results = google_search(company_name, num=100, stop=100, pause=2.0)\n",
    "\n",
    "    # Iterate through the search results and process the articles\n",
    "    for url in search_results:\n",
    "        # Check if the URL is an article link\n",
    "        if '/article/' in url:\n",
    "            keyword_counts = get_article_keywords(url)\n",
    "\n",
    "            if keyword_counts is None:\n",
    "                continue\n",
    "\n",
    "            good_news_count, bad_news_count = keyword_counts\n",
    "\n",
    "            # Print the results for each article\n",
    "            print(f\"Article URL: {url}\")\n",
    "            print(f\"Good News Count: {good_news_count}\")\n",
    "            print(f\"Bad News Count: {bad_news_count}\")\n",
    "            print('---')\n",
    "\n",
    "def get_company_names():\n",
    "    url = \"https://stockanalysis.com/list/biggest-companies/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    names = []\n",
    "\n",
    "    # Find all the table cells with the specified class\n",
    "    cells = soup.find_all(\"td\", class_=\"slw svelte-1tv1ofl\")\n",
    "    for cell in cells:\n",
    "        # Extract the text content of each cell (company name)\n",
    "        name = cell.text.strip()\n",
    "        names.append(name)\n",
    "\n",
    "    return names[:1000]\n",
    "\n",
    "# Get the list of company names\n",
    "company_names = get_company_names()\n",
    "\n",
    "# Get the list of top 1000 companies\n",
    "company_list = get_company_names()\n",
    "\n",
    "# Iterate through the company list and scrape articles for each company\n",
    "for company in company_list:\n",
    "    scrape_articles(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "521b0cc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 429: Too Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 84>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Iterate through the company list and scrape articles for each company\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m company \u001b[38;5;129;01min\u001b[39;00m company_list:\n\u001b[1;32m---> 85\u001b[0m     \u001b[43mscrape_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36mscrape_articles\u001b[1;34m(company_name)\u001b[0m\n\u001b[0;32m     37\u001b[0m search_results \u001b[38;5;241m=\u001b[39m google_search(company_name, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, pause\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Iterate through the search results and process the articles\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m search_results:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Check if the URL is an article link\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/article/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m url:\n\u001b[0;32m     43\u001b[0m         keyword_counts \u001b[38;5;241m=\u001b[39m get_article_keywords(url)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:305\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(query, tld, lang, tbs, safe, num, start, stop, pause, country, extra_params, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    302\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(pause)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# Request the Google Search results page.\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mget_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_ssl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Parse the response and get every anchored URL.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bs4:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:174\u001b[0m, in \u001b[0;36mget_page\u001b[1;34m(url, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    172\u001b[0m cookie_jar\u001b[38;5;241m.\u001b[39madd_cookie_header(request)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_ssl:\n\u001b[1;32m--> 174\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     context \u001b[38;5;241m=\u001b[39m ssl\u001b[38;5;241m.\u001b[39m_create_unverified_context()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:555\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    553\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    554\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[1;32m--> 555\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:747\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    744\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    745\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 429: Too Many Requests"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search as google_search\n",
    "import yfinance as yf\n",
    "import time\n",
    "\n",
    "def get_article_keywords(url):\n",
    "    # Define the keywords related to good news and bad news\n",
    "    good_news_keywords = ['positive', 'improvement', 'growth', 'increase']\n",
    "    bad_news_keywords = ['negative', 'decline', 'loss', 'decrease']\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the article URL\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content of the article\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get the text content of the article\n",
    "        article_text = soup.get_text().lower()\n",
    "\n",
    "        # Count the occurrences of good news and bad news keywords\n",
    "        good_news_count = sum(article_text.count(keyword) for keyword in good_news_keywords)\n",
    "        bad_news_count = sum(article_text.count(keyword) for keyword in bad_news_keywords)\n",
    "\n",
    "        return good_news_count, bad_news_count\n",
    "\n",
    "    except (requests.RequestException, ValueError, AttributeError) as e:\n",
    "        print(f\"Error accessing article: {url}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_articles(company_name):\n",
    "    # Perform a Google search for the company name\n",
    "    search_results = google_search(company_name, num=100, stop=100, pause=2.0)\n",
    "\n",
    "    # Iterate through the search results and process the articles\n",
    "    for url in search_results:\n",
    "        # Check if the URL is an article link\n",
    "        if '/article/' in url:\n",
    "            keyword_counts = get_article_keywords(url)\n",
    "\n",
    "            if keyword_counts is None:\n",
    "                continue\n",
    "\n",
    "            good_news_count, bad_news_count = keyword_counts\n",
    "\n",
    "            # Print the results for each article\n",
    "            print(f\"Article URL: {url}\")\n",
    "            print(f\"Good News Count: {good_news_count}\")\n",
    "            print(f\"Bad News Count: {bad_news_count}\")\n",
    "            print('---')\n",
    "        \n",
    "        # Introduce a delay of 1 second between each request\n",
    "        time.sleep(1)\n",
    "\n",
    "def get_company_names():\n",
    "    url = \"https://stockanalysis.com/list/biggest-companies/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    names = []\n",
    "\n",
    "    # Find all the table cells with the specified class\n",
    "    cells = soup.find_all(\"td\", class_=\"slw svelte-1tv1ofl\")\n",
    "    for cell in cells:\n",
    "        # Extract the text content of each cell (company name)\n",
    "        name = cell.text.strip()\n",
    "        names.append(name)\n",
    "\n",
    "    return names[:1]\n",
    "\n",
    "# Get the list of company names\n",
    "company_names = get_company_names()\n",
    "\n",
    "# Get the list of top 1000 companies\n",
    "company_list = get_company_names()\n",
    "\n",
    "# Iterate through the company list and scrape articles for each company\n",
    "for company in company_list:\n",
    "    scrape_articles(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7019d498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc.\n",
      "Microsoft Corporation\n",
      "Alphabet Inc.\n",
      "Amazon.com, Inc.\n",
      "NVIDIA Corporation\n",
      "Tesla, Inc.\n",
      "Berkshire Hathaway Inc.\n",
      "Meta Platforms, Inc.\n",
      "Taiwan Semiconductor Manufacturing Company Limited\n",
      "Visa Inc.\n",
      "UnitedHealth Group Incorporated\n",
      "Exxon Mobil Corporation\n",
      "Eli Lilly and Company\n",
      "Walmart Inc.\n",
      "Johnson & Johnson\n",
      "JPMorgan Chase & Co.\n",
      "Broadcom Inc.\n",
      "Novo Nordisk A/S\n",
      "Mastercard Incorporated\n",
      "The Procter & Gamble Company\n",
      "Oracle Corporation\n",
      "The Home Depot, Inc.\n",
      "Chevron Corporation\n",
      "ASML Holding N.V.\n",
      "Merck & Co., Inc.\n",
      "The Coca-Cola Company\n",
      "PepsiCo, Inc.\n",
      "AbbVie Inc.\n",
      "Costco Wholesale Corporation\n",
      "Bank of America Corporation\n",
      "Alibaba Group Holding Limited\n",
      "BHP Group Limited\n",
      "AstraZeneca PLC\n",
      "Toyota Motor Corporation\n",
      "Pfizer Inc.\n",
      "Adobe Inc.\n",
      "Novartis AG\n",
      "McDonald's Corporation\n",
      "Cisco Systems, Inc.\n",
      "Advanced Micro Devices, Inc.\n",
      "Salesforce, Inc.\n",
      "Shell plc\n",
      "Thermo Fisher Scientific Inc.\n",
      "Accenture plc\n",
      "Fomento Económico Mexicano, SAB de CV\n",
      "Netflix, Inc.\n",
      "Linde plc\n",
      "Abbott Laboratories\n",
      "Danaher Corporation\n",
      "NIKE, Inc.\n",
      "Comcast Corporation\n",
      "The Walt Disney Company\n",
      "Texas Instruments Incorporated\n",
      "SAP SE\n",
      "Wells Fargo & Company\n",
      "HSBC Holdings plc\n",
      "T-Mobile US, Inc.\n",
      "Verizon Communications Inc.\n",
      "United Parcel Service, Inc.\n",
      "NextEra Energy, Inc.\n",
      "Intel Corporation\n",
      "Morgan Stanley\n",
      "Philip Morris International Inc.\n",
      "Raytheon Technologies Corporation\n",
      "TotalEnergies SE\n",
      "QUALCOMM Incorporated\n",
      "Bristol-Myers Squibb Company\n",
      "Honeywell International Inc.\n",
      "The Boeing Company\n",
      "Royal Bank of Canada\n",
      "American Express Company\n",
      "Sanofi\n",
      "Unilever PLC\n",
      "S&P Global Inc.\n",
      "Lowe's Companies, Inc.\n",
      "Caterpillar Inc.\n",
      "Intuit Inc.\n",
      "International Business Machines Corporation\n",
      "ConocoPhillips\n",
      "Union Pacific Corporation\n",
      "Sony Group Corporation\n",
      "HDFC Bank Limited\n",
      "Amgen Inc.\n",
      "Applied Materials, Inc.\n",
      "Deere & Company\n",
      "Medtronic plc\n",
      "ServiceNow, Inc.\n",
      "Starbucks Corporation\n",
      "Lockheed Martin Corporation\n",
      "General Electric Company\n",
      "AT&T Inc.\n",
      "Intuitive Surgical, Inc.\n",
      "Prologis, Inc.\n",
      "The Goldman Sachs Group, Inc.\n",
      "Anheuser-Busch InBev SA/NV\n",
      "Stryker Corporation\n",
      "The Toronto-Dominion Bank\n",
      "Rio Tinto Group\n",
      "Blackstone Inc.\n",
      "PDD Holdings Inc.\n",
      "BlackRock, Inc.\n",
      "Elevance Health Inc.\n",
      "BP p.l.c.\n",
      "Mondelez International, Inc.\n",
      "Gilead Sciences, Inc.\n",
      "Diageo plc\n",
      "Booking Holdings Inc.\n",
      "Analog Devices, Inc.\n",
      "The Charles Schwab Corporation\n",
      "Citigroup Inc.\n",
      "Equinor ASA\n",
      "The TJX Companies, Inc.\n",
      "Petróleo Brasileiro S.A. - Petrobras\n",
      "American Tower Corporation\n",
      "Automatic Data Processing, Inc.\n",
      "Marsh & McLennan Companies, Inc.\n",
      "Vertex Pharmaceuticals Incorporated\n",
      "CVS Health Corporation\n",
      "Lam Research Corporation\n",
      "Shopify Inc.\n",
      "Mitsubishi UFJ Financial Group, Inc.\n",
      "Uber Technologies, Inc.\n",
      "Regeneron Pharmaceuticals, Inc.\n",
      "Petróleo Brasileiro S.A. - Petrobras\n",
      "ICICI Bank Limited\n",
      "Airbnb, Inc.\n",
      "Altria Group, Inc.\n",
      "Chubb Limited\n",
      "HCA Healthcare, Inc.\n",
      "Cigna Corporation\n",
      "Canadian National Railway Company\n",
      "Eaton Corporation plc\n",
      "The Southern Company\n",
      "Boston Scientific Corporation\n",
      "Zoetis Inc.\n",
      "Micron Technology, Inc.\n",
      "Enbridge Inc.\n",
      "Illinois Tool Works Inc.\n",
      "The Progressive Corporation\n",
      "Palo Alto Networks, Inc.\n",
      "Equinix, Inc.\n",
      "Canadian Pacific Railway Limited\n",
      "British American Tobacco p.l.c.\n",
      "Becton, Dickinson and Company\n",
      "PayPal Holdings, Inc.\n",
      "América Móvil, SAB de CV\n",
      "GSK plc\n",
      "Fiserv, Inc.\n",
      "Duke Energy Corporation\n",
      "The Estée Lauder Companies Inc.\n",
      "Northrop Grumman Corporation\n",
      "Synopsys, Inc.\n",
      "Schlumberger Limited\n",
      "CSX Corporation\n",
      "Waste Management, Inc.\n",
      "Aon plc\n",
      "KLA Corporation\n",
      "KKR & Co. Inc.\n",
      "CME Group Inc.\n",
      "EOG Resources, Inc.\n",
      "Infosys Limited\n",
      "Cadence Design Systems, Inc.\n",
      "Air Products and Chemicals, Inc.\n",
      "Activision Blizzard, Inc.\n",
      "UBS Group AG\n",
      "NetEase, Inc.\n",
      "The Sherwin-Williams Company\n",
      "Bank of Montreal\n",
      "Colgate-Palmolive Company\n",
      "Moody's Corporation\n",
      "Vale S.A.\n",
      "Intercontinental Exchange, Inc.\n",
      "RELX PLC\n",
      "Target Corporation\n",
      "Monster Beverage Corporation\n",
      "MercadoLibre, Inc.\n",
      "VMware, Inc.\n",
      "Canadian Natural Resources Limited\n",
      "JD.com, Inc.\n",
      "Thomson Reuters Corporation\n",
      "The Bank of Nova Scotia\n",
      "Snowflake Inc.\n",
      "General Dynamics Corporation\n",
      "FedEx Corporation\n",
      "Freeport-McMoRan Inc.\n",
      "Southern Copper Corporation\n",
      "Chipotle Mexican Grill, Inc.\n",
      "Humana Inc.\n",
      "Itaú Unibanco Holding S.A.\n",
      "Banco Santander, S.A.\n",
      "Workday, Inc.\n",
      "Enterprise Products Partners L.P.\n",
      "3M Company\n",
      "Fortinet, Inc.\n",
      "Ford Motor Company\n",
      "Sumitomo Mitsui Financial Group, Inc.\n",
      "O'Reilly Automotive, Inc.\n",
      "Marriott International, Inc.\n",
      "Ferrari N.V.\n",
      "Edwards Lifesciences Corporation\n",
      "Marvell Technology, Inc.\n",
      "Honda Motor Co., Ltd.\n",
      "The Hershey Company\n",
      "Stellantis N.V.\n",
      "McKesson Corporation\n",
      "Occidental Petroleum Corporation\n",
      "General Motors Company\n",
      "Brookfield Corporation\n",
      "Arista Networks, Inc.\n",
      "Baidu, Inc.\n",
      "Ecolab Inc.\n",
      "NXP Semiconductors N.V.\n",
      "The PNC Financial Services Group, Inc.\n",
      "Charter Communications, Inc.\n",
      "Norfolk Southern Corporation\n",
      "Takeda Pharmaceutical Company Limited\n",
      "Public Storage\n",
      "Crown Castle Inc.\n",
      "Ambev S.A.\n",
      "Cintas Corporation\n",
      "DexCom, Inc.\n",
      "U.S. Bancorp\n",
      "Emerson Electric Co.\n",
      "Roper Technologies, Inc.\n",
      "National Grid plc\n",
      "Kenvue Inc.\n",
      "Amphenol Corporation\n",
      "Moderna, Inc.\n",
      "Marathon Petroleum Corporation\n",
      "Lululemon Athletica Inc.\n",
      "ING Groep N.V.\n",
      "General Mills, Inc.\n",
      "Microchip Technology Incorporated\n",
      "Parker-Hannifin Corporation\n",
      "Pioneer Natural Resources Company\n",
      "Eni S.p.A.\n",
      "Motorola Solutions, Inc.\n",
      "Atlassian Corporation Plc\n",
      "Sempra\n",
      "Banco Santander (Brasil) S.A.\n",
      "Las Vegas Sands Corp.\n",
      "Autodesk, Inc.\n",
      "Kimberly-Clark Corporation\n",
      "Republic Services, Inc.\n",
      "Constellation Brands, Inc.\n",
      "STMicroelectronics N.V.\n",
      "The Kraft Heinz Company\n",
      "Arthur J. Gallagher & Co.\n",
      "Apollo Global Management, Inc.\n",
      "Keurig Dr Pepper Inc.\n",
      "TransDigm Group Incorporated\n",
      "Woodside Energy Group Ltd\n",
      "Johnson Controls International plc\n",
      "Dominion Energy, Inc.\n",
      "Phillips 66\n",
      "AutoZone, Inc.\n",
      "Banco Bilbao Vizcaya Argentaria, S.A.\n",
      "Biogen Inc.\n",
      "American Electric Power Company, Inc.\n",
      "Capital One Financial Corporation\n",
      "PG&E Corporation\n",
      "Truist Financial Corporation\n",
      "Aflac Incorporated\n",
      "TE Connectivity Ltd.\n",
      "Simon Property Group, Inc.\n",
      "Trane Technologies plc\n",
      "MetLife, Inc.\n",
      "BCE Inc.\n",
      "Copart, Inc.\n",
      "Realty Income Corporation\n",
      "Hess Corporation\n",
      "TC Energy Corporation\n",
      "PACCAR Inc\n",
      "Corteva, Inc.\n",
      "Welltower Inc.\n",
      "Valero Energy Corporation\n",
      "American International Group, Inc.\n",
      "Paychex, Inc.\n",
      "The Travelers Companies, Inc.\n",
      "ON Semiconductor Corporation\n",
      "Exelon Corporation\n",
      "Archer-Daniels-Midland Company\n",
      "Energy Transfer LP\n",
      "Canadian Imperial Bank of Commerce\n",
      "Prudential plc\n",
      "IQVIA Holdings Inc.\n",
      "Alcon Inc.\n",
      "Suncor Energy Inc.\n",
      "MSCI Inc.\n",
      "D.R. Horton, Inc.\n",
      "IDEXX Laboratories, Inc.\n",
      "Carrier Global Corporation\n",
      "Block, Inc.\n",
      "Mizuho Financial Group, Inc.\n",
      "Yum! Brands, Inc.\n",
      "CRH plc\n",
      "Kinder Morgan, Inc.\n",
      "Hilton Worldwide Holdings Inc.\n",
      "Haleon plc\n",
      "Nucor Corporation\n",
      "The Trade Desk, Inc.\n",
      "Dow Inc.\n",
      "Banco Bradesco S.A.\n",
      "Sysco Corporation\n",
      "Lloyds Banking Group plc\n",
      "Seagen Inc.\n",
      "The Williams Companies, Inc.\n",
      "CrowdStrike Holdings, Inc.\n",
      "W.W. Grainger, Inc.\n",
      "AmerisourceBergen Corporation\n",
      "Sea Limited\n",
      "Ross Stores, Inc.\n",
      "L3Harris Technologies, Inc.\n",
      "Otis Worldwide Corporation\n",
      "Rockwell Automation, Inc.\n",
      "Dell Technologies Inc.\n",
      "Cheniere Energy, Inc.\n",
      "GE HealthCare Technologies Inc.\n",
      "Dollar General Corporation\n",
      "Centene Corporation\n",
      "AMETEK, Inc.\n",
      "Manulife Financial Corporation\n",
      "Agilent Technologies, Inc.\n",
      "Waste Connections, Inc.\n",
      "Old Dominion Freight Line, Inc.\n",
      "Electronic Arts Inc.\n",
      "The Bank of New York Mellon Corporation\n",
      "Restaurant Brands International Inc.\n",
      "Nu Holdings Ltd.\n",
      "GLOBALFOUNDRIES Inc.\n",
      "Xcel Energy Inc.\n",
      "The Kroger Co.\n",
      "CoStar Group, Inc.\n",
      "MPLX LP\n",
      "Palantir Technologies Inc.\n",
      "Ameriprise Financial, Inc.\n",
      "Newmont Corporation\n",
      "Warner Bros. Discovery, Inc.\n",
      "Interactive Brokers Group, Inc.\n",
      "Cummins Inc.\n",
      "PPG Industries, Inc.\n",
      "Li Auto Inc.\n",
      "Mobileye Global Inc.\n",
      "Lennar Corporation\n",
      "VICI Properties Inc.\n",
      "Fidelity National Information Services, Inc.\n",
      "Illumina, Inc.\n",
      "Cognizant Technology Solutions Corporation\n",
      "DuPont de Nemours, Inc.\n",
      "Consolidated Edison, Inc.\n",
      "Verisk Analytics, Inc.\n",
      "ResMed Inc.\n",
      "Chunghwa Telecom Co., Ltd.\n",
      "Cenovus Energy Inc.\n",
      "Brown-Forman Corporation\n",
      "Fastenal Company\n",
      "Devon Energy Corporation\n",
      "Prudential Financial, Inc.\n",
      "Veeva Systems Inc.\n",
      "Datadog, Inc.\n",
      "Barclays PLC\n",
      "Public Service Enterprise Group Incorporated\n",
      "Digital Realty Trust, Inc.\n",
      "Ferguson plc\n",
      "HP Inc.\n",
      "Dollar Tree, Inc.\n",
      "Baker Hughes Company\n",
      "Constellation Energy Corporation\n",
      "NatWest Group plc\n",
      "Coca-Cola Europacific Partners PLC\n",
      "Coupang, Inc.\n",
      "Halliburton Company\n",
      "Zimmer Biomet Holdings, Inc.\n",
      "Discover Financial Services\n",
      "Orange S.A.\n",
      "Sun Life Financial Inc.\n",
      "Barrick Gold Corporation\n",
      "LyondellBasell Industries N.V.\n",
      "Spotify Technology S.A.\n",
      "Keysight Technologies, Inc.\n",
      "The Allstate Corporation\n",
      "Imperial Oil Limited\n",
      "Nutrien Ltd.\n",
      "ANSYS, Inc.\n",
      "Mettler-Toledo International Inc.\n",
      "WEC Energy Group, Inc.\n",
      "Ares Management Corporation\n",
      "American Water Works Company, Inc.\n",
      "Corning Incorporated\n",
      "Equifax Inc.\n",
      "United Rentals, Inc.\n",
      "Gartner, Inc.\n",
      "DoorDash, Inc.\n",
      "Franco-Nevada Corporation\n",
      "TELUS Corporation\n",
      "Vulcan Materials Company\n",
      "Aptiv PLC\n",
      "Walgreens Boots Alliance, Inc.\n",
      "Delta Air Lines, Inc.\n",
      "ONEOK, Inc.\n",
      "AvalonBay Communities, Inc.\n",
      "Quanta Services, Inc.\n",
      "Xylem Inc.\n",
      "Symbotic Inc.\n",
      "BioNTech SE\n",
      "Albemarle Corporation\n",
      "Perusahaan Perseroan (Persero) PT Telekomunikasi Indonesia Tbk\n",
      "Global Payments Inc.\n",
      "MongoDB, Inc.\n",
      "Martin Marietta Materials, Inc.\n",
      "West Pharmaceutical Services, Inc.\n",
      "Edison International\n",
      "Arch Capital Group Ltd.\n",
      "Ingersoll Rand Inc.\n",
      "Wipro Limited\n",
      "Monolithic Power Systems, Inc.\n",
      "Equity Residential\n",
      "Nasdaq, Inc.\n",
      "Genmab A/S\n",
      "HubSpot, Inc.\n",
      "T. Rowe Price Group, Inc.\n",
      "Vodafone Group PLC\n",
      "SBA Communications Corporation\n",
      "Agnico Eagle Mines Limited\n",
      "Alnylam Pharmaceuticals, Inc.\n",
      "Yum China Holdings, Inc.\n",
      "Align Technology, Inc.\n",
      "CGI Inc.\n",
      "Enphase Energy, Inc.\n",
      "State Street Corporation\n",
      "Fortive Corporation\n",
      "Ryanair Holdings plc\n",
      "McCormick & Company, Incorporated\n",
      "Eversource Energy\n",
      "Roblox Corporation\n",
      "CBRE Group, Inc.\n",
      "Trip.com Group Limited\n",
      "Willis Towers Watson PLC\n",
      "Royal Caribbean Cruises Ltd.\n",
      "eBay Inc.\n",
      "Tractor Supply Company\n",
      "CDW Corporation\n",
      "UDR, Inc.\n",
      "ArcelorMittal S.A.\n",
      "POSCO Holdings Inc.\n",
      "Rogers Communications Inc.\n",
      "Church & Dwight Co., Inc.\n",
      "DTE Energy Company\n",
      "Take-Two Interactive Software, Inc.\n",
      "Nokia Oyj\n",
      "Horizon Therapeutics PLC\n",
      "Zscaler, Inc.\n",
      "VeriSign, Inc.\n",
      "Diamondback Energy, Inc.\n",
      "Cloudflare, Inc.\n",
      "Kellogg Company\n",
      "ZTO Express (Cayman) Inc.\n",
      "Telefónica, S.A.\n",
      "Hormel Foods Corporation\n",
      "Weyerhaeuser Company\n",
      "Genuine Parts Company\n",
      "FirstEnergy Corp.\n",
      "United Microelectronics Corporation\n",
      "Cardinal Health, Inc.\n",
      "Ulta Beauty, Inc.\n",
      "Companhia Paranaense de Energia - COPEL\n",
      "Teck Resources Limited\n",
      "Baxter International Inc.\n",
      "Cheniere Energy Partners, L.P.\n",
      "The Hartford Financial Services Group, Inc.\n",
      "argenx SE\n",
      "BeiGene, Ltd.\n",
      "Deutsche Bank Aktiengesellschaft\n",
      "Ameren Corporation\n",
      "Hewlett Packard Enterprise Company\n",
      "KE Holdings Inc.\n",
      "Entergy Corporation\n",
      "Alexandria Real Estate Equities, Inc.\n",
      "Ecopetrol S.A.\n",
      "STERIS plc\n",
      "Invitation Homes Inc.\n",
      "Raymond James Financial, Inc.\n",
      "First Solar, Inc.\n",
      "Fortis Inc.\n",
      "Rentokil Initial plc\n",
      "Extra Space Storage Inc.\n",
      "Garmin Ltd.\n",
      "Zoom Video Communications, Inc.\n",
      "Meta Data Limited\n",
      "Carnival Corporation & plc\n",
      "Dover Corporation\n",
      "Darden Restaurants, Inc.\n",
      "Sociedad Química y Minera de Chile S.A.\n",
      "M&T Bank Corporation\n",
      "Wheaton Precious Metals Corp.\n",
      "Rollins, Inc.\n",
      "Laboratory Corporation of America Holdings\n",
      "ORIX Corporation\n",
      "Live Nation Entertainment, Inc.\n",
      "Southwest Airlines Co.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_company_names():\n",
    "    url = \"https://stockanalysis.com/list/biggest-companies/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    names = []\n",
    "\n",
    "    # Find all the table cells with the specified class\n",
    "    cells = soup.find_all(\"td\", class_=\"slw svelte-1tv1ofl\")\n",
    "    for cell in cells:\n",
    "        # Extract the text content of each cell (company name)\n",
    "        name = cell.text.strip()\n",
    "        names.append(name)\n",
    "\n",
    "    return names[:1000]\n",
    "\n",
    "# Get the list of company names\n",
    "company_names = get_company_names()\n",
    "\n",
    "# Print the company names\n",
    "for name in company_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6eddcd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 429: Too Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 82>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Iterate through the company list and scrape articles for each company\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m company \u001b[38;5;129;01min\u001b[39;00m company_list:\n\u001b[1;32m---> 83\u001b[0m     \u001b[43mscrape_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36mscrape_articles\u001b[1;34m(company_name)\u001b[0m\n\u001b[0;32m     37\u001b[0m search_results \u001b[38;5;241m=\u001b[39m google_search(company_name, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, pause\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Iterate through the search results and process the articles\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m search_results:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Check if the URL is an article link\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/article/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m url:\n\u001b[0;32m     43\u001b[0m         keyword_counts \u001b[38;5;241m=\u001b[39m get_article_keywords(url)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:305\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(query, tld, lang, tbs, safe, num, start, stop, pause, country, extra_params, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    302\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(pause)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# Request the Google Search results page.\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mget_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_ssl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Parse the response and get every anchored URL.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bs4:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:174\u001b[0m, in \u001b[0;36mget_page\u001b[1;34m(url, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    172\u001b[0m cookie_jar\u001b[38;5;241m.\u001b[39madd_cookie_header(request)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_ssl:\n\u001b[1;32m--> 174\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     context \u001b[38;5;241m=\u001b[39m ssl\u001b[38;5;241m.\u001b[39m_create_unverified_context()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:555\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    553\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    554\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[1;32m--> 555\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:747\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    744\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    745\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 429: Too Many Requests"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search as google_search\n",
    "import yfinance as yf\n",
    "import time\n",
    "\n",
    "def get_article_keywords(url):\n",
    "    # Define the keywords related to good news and bad news\n",
    "    good_news_keywords = ['positive', 'improvement', 'growth', 'increase']\n",
    "    bad_news_keywords = ['negative', 'decline', 'loss', 'decrease']\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the article URL\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content of the article\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get the text content of the article\n",
    "        article_text = soup.get_text().lower()\n",
    "\n",
    "        # Count the occurrences of good news and bad news keywords\n",
    "        good_news_count = sum(article_text.count(keyword) for keyword in good_news_keywords)\n",
    "        bad_news_count = sum(article_text.count(keyword) for keyword in bad_news_keywords)\n",
    "\n",
    "        return good_news_count, bad_news_count\n",
    "\n",
    "    except (requests.RequestException, ValueError, AttributeError) as e:\n",
    "        print(f\"Error accessing article: {url}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_articles(company_name):\n",
    "    # Perform a Google search for the company name\n",
    "    search_results = google_search(company_name, num=100, stop=100, pause=2.0)\n",
    "\n",
    "    # Iterate through the search results and process the articles\n",
    "    for url in search_results:\n",
    "        # Check if the URL is an article link\n",
    "        if '/article/' in url:\n",
    "            keyword_counts = get_article_keywords(url)\n",
    "\n",
    "            if keyword_counts is None:\n",
    "                continue\n",
    "\n",
    "            good_news_count, bad_news_count = keyword_counts\n",
    "\n",
    "            # Print the results for each article\n",
    "            print(f\"Article URL: {url}\")\n",
    "            print(f\"Good News Count: {good_news_count}\")\n",
    "            print(f\"Bad News Count: {bad_news_count}\")\n",
    "            print('---')\n",
    "    \n",
    "\n",
    "def get_company_names():\n",
    "    url = \"https://stockanalysis.com/list/biggest-companies/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    names = []\n",
    "\n",
    "    # Find all the table cells with the specified class\n",
    "    cells = soup.find_all(\"td\", class_=\"slw svelte-1tv1ofl\")\n",
    "    for cell in cells:\n",
    "        # Extract the text content of each cell (company name)\n",
    "        name = cell.text.strip()\n",
    "        names.append(name)\n",
    "\n",
    "    return names[:1]\n",
    "\n",
    "# Get the list of company names\n",
    "company_names = get_company_names()\n",
    "\n",
    "# Get the list of top 1000 companies\n",
    "company_list = get_company_names()\n",
    "\n",
    "# Iterate through the company list and scrape articles for each company\n",
    "for company in company_list:\n",
    "    scrape_articles(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a909dad",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 429: Too Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 79>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Iterate through the company list and scrape articles for each company\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m company \u001b[38;5;129;01min\u001b[39;00m company_names:\n\u001b[1;32m---> 80\u001b[0m     \u001b[43mscrape_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mscrape_articles\u001b[1;34m(company_name)\u001b[0m\n\u001b[0;32m     37\u001b[0m search_results \u001b[38;5;241m=\u001b[39m google_search(company_name, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, pause\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Iterate through the search results and process the articles\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m search_results:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Check if the URL is an article link\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/article/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m url:\n\u001b[0;32m     43\u001b[0m         keyword_counts \u001b[38;5;241m=\u001b[39m get_article_keywords(url)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:305\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(query, tld, lang, tbs, safe, num, start, stop, pause, country, extra_params, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    302\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(pause)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# Request the Google Search results page.\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mget_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_ssl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Parse the response and get every anchored URL.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bs4:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:174\u001b[0m, in \u001b[0;36mget_page\u001b[1;34m(url, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    172\u001b[0m cookie_jar\u001b[38;5;241m.\u001b[39madd_cookie_header(request)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_ssl:\n\u001b[1;32m--> 174\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     context \u001b[38;5;241m=\u001b[39m ssl\u001b[38;5;241m.\u001b[39m_create_unverified_context()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:555\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    553\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    554\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[1;32m--> 555\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:747\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    744\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    745\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 429: Too Many Requests"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search as google_search\n",
    "import yfinance as yf\n",
    "import time\n",
    "\n",
    "def get_article_keywords(url):\n",
    "    # Define the keywords related to good news and bad news\n",
    "    good_news_keywords = ['positive', 'improvement', 'growth', 'increase']\n",
    "    bad_news_keywords = ['negative', 'decline', 'loss', 'decrease']\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the article URL\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content of the article\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get the text content of the article\n",
    "        article_text = soup.get_text().lower()\n",
    "\n",
    "        # Count the occurrences of good news and bad news keywords\n",
    "        good_news_count = sum(article_text.count(keyword) for keyword in good_news_keywords)\n",
    "        bad_news_count = sum(article_text.count(keyword) for keyword in bad_news_keywords)\n",
    "\n",
    "        return good_news_count, bad_news_count\n",
    "\n",
    "    except (requests.RequestException, ValueError, AttributeError) as e:\n",
    "        print(f\"Error accessing article: {url}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_articles(company_name):\n",
    "    # Perform a Google search for the company name\n",
    "    search_results = google_search(company_name, num=100, stop=100, pause=2.0)\n",
    "\n",
    "    # Iterate through the search results and process the articles\n",
    "    for url in search_results:\n",
    "        # Check if the URL is an article link\n",
    "        if '/article/' in url:\n",
    "            keyword_counts = get_article_keywords(url)\n",
    "\n",
    "            if keyword_counts is None:\n",
    "                continue\n",
    "\n",
    "            good_news_count, bad_news_count = keyword_counts\n",
    "\n",
    "            # Print the results for each article\n",
    "            print(f\"Article URL: {url}\")\n",
    "            print(f\"Good News Count: {good_news_count}\")\n",
    "            print(f\"Bad News Count: {bad_news_count}\")\n",
    "            print('---')\n",
    "            time.sleep(1)  # Delay for 1 second between requests\n",
    "\n",
    "def get_company_names():\n",
    "    url = \"https://stockanalysis.com/list/biggest-companies/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    names = []\n",
    "\n",
    "    # Find all the table cells with the specified class\n",
    "    cells = soup.find_all(\"td\", class_=\"slw svelte-1tv1ofl\")\n",
    "    for cell in cells:\n",
    "        # Extract the text content of each cell (company name)\n",
    "        name = cell.text.strip()\n",
    "        names.append(name)\n",
    "\n",
    "    return names[:10]  # Retrieve only the top 10 companies\n",
    "\n",
    "# Get the list of company names\n",
    "company_names = get_company_names()\n",
    "\n",
    "# Iterate through the company list and scrape articles for each company\n",
    "for company in company_names:\n",
    "    scrape_articles(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a749d3c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 429: Too Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 79>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Iterate through the company list and scrape articles for each company\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m company \u001b[38;5;129;01min\u001b[39;00m company_names:\n\u001b[1;32m---> 80\u001b[0m     \u001b[43mscrape_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36mscrape_articles\u001b[1;34m(company_name)\u001b[0m\n\u001b[0;32m     37\u001b[0m search_results \u001b[38;5;241m=\u001b[39m google_search(company_name, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, pause\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Iterate through the search results and process the articles\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m search_results:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Check if the URL is an article link\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/article/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m url:\n\u001b[0;32m     43\u001b[0m         keyword_counts \u001b[38;5;241m=\u001b[39m get_article_keywords(url)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:305\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(query, tld, lang, tbs, safe, num, start, stop, pause, country, extra_params, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    302\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(pause)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# Request the Google Search results page.\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mget_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_ssl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Parse the response and get every anchored URL.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bs4:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:174\u001b[0m, in \u001b[0;36mget_page\u001b[1;34m(url, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    172\u001b[0m cookie_jar\u001b[38;5;241m.\u001b[39madd_cookie_header(request)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_ssl:\n\u001b[1;32m--> 174\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     context \u001b[38;5;241m=\u001b[39m ssl\u001b[38;5;241m.\u001b[39m_create_unverified_context()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:555\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    553\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    554\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[1;32m--> 555\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:747\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    744\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    745\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 429: Too Many Requests"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search as google_search\n",
    "import yfinance as yf\n",
    "import time\n",
    "\n",
    "def get_article_keywords(url):\n",
    "    # Define the keywords related to good news and bad news\n",
    "    good_news_keywords = ['positive', 'improvement', 'growth', 'increase']\n",
    "    bad_news_keywords = ['negative', 'decline', 'loss', 'decrease']\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the article URL\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content of the article\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get the text content of the article\n",
    "        article_text = soup.get_text().lower()\n",
    "\n",
    "        # Count the occurrences of good news and bad news keywords\n",
    "        good_news_count = sum(article_text.count(keyword) for keyword in good_news_keywords)\n",
    "        bad_news_count = sum(article_text.count(keyword) for keyword in bad_news_keywords)\n",
    "\n",
    "        return good_news_count, bad_news_count\n",
    "\n",
    "    except (requests.RequestException, ValueError, AttributeError) as e:\n",
    "        print(f\"Error accessing article: {url}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_articles(company_name):\n",
    "    # Perform a Google search for the company name\n",
    "    search_results = google_search(company_name, num=100, stop=100, pause=5.0)\n",
    "\n",
    "    # Iterate through the search results and process the articles\n",
    "    for url in search_results:\n",
    "        # Check if the URL is an article link\n",
    "        if '/article/' in url:\n",
    "            keyword_counts = get_article_keywords(url)\n",
    "\n",
    "            if keyword_counts is None:\n",
    "                continue\n",
    "\n",
    "            good_news_count, bad_news_count = keyword_counts\n",
    "\n",
    "            # Print the results for each article\n",
    "            print(f\"Article URL: {url}\")\n",
    "            print(f\"Good News Count: {good_news_count}\")\n",
    "            print(f\"Bad News Count: {bad_news_count}\")\n",
    "            print('---')\n",
    "            time.sleep(5)  # Delay for 5 seconds between requests\n",
    "\n",
    "def get_company_names():\n",
    "    url = \"https://stockanalysis.com/list/biggest-companies/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    names = []\n",
    "\n",
    "    # Find all the table cells with the specified class\n",
    "    cells = soup.find_all(\"td\", class_=\"slw svelte-1tv1ofl\")\n",
    "    for cell in cells:\n",
    "        # Extract the text content of each cell (company name)\n",
    "        name = cell.text.strip()\n",
    "        names.append(name)\n",
    "\n",
    "    return names[:10]  # Retrieve only the top 10 companies\n",
    "\n",
    "# Get the list of company names\n",
    "company_names = get_company_names()\n",
    "\n",
    "# Iterate through the company list and scrape articles for each company\n",
    "for company in company_names:\n",
    "    scrape_articles(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1790debe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc.\n",
      "Microsoft Corporation\n",
      "Alphabet Inc.\n",
      "Amazon.com, Inc.\n",
      "NVIDIA Corporation\n",
      "Tesla, Inc.\n",
      "Berkshire Hathaway Inc.\n",
      "Meta Platforms, Inc.\n",
      "Taiwan Semiconductor Manufacturing Company Limited\n",
      "Visa Inc.\n",
      "UnitedHealth Group Incorporated\n",
      "Exxon Mobil Corporation\n",
      "Eli Lilly and Company\n",
      "Walmart Inc.\n",
      "Johnson & Johnson\n",
      "JPMorgan Chase & Co.\n",
      "Broadcom Inc.\n",
      "Novo Nordisk A/S\n",
      "Mastercard Incorporated\n",
      "The Procter & Gamble Company\n",
      "Oracle Corporation\n",
      "The Home Depot, Inc.\n",
      "Chevron Corporation\n",
      "ASML Holding N.V.\n",
      "Merck & Co., Inc.\n",
      "The Coca-Cola Company\n",
      "PepsiCo, Inc.\n",
      "AbbVie Inc.\n",
      "Costco Wholesale Corporation\n",
      "Bank of America Corporation\n",
      "Alibaba Group Holding Limited\n",
      "BHP Group Limited\n",
      "AstraZeneca PLC\n",
      "Toyota Motor Corporation\n",
      "Pfizer Inc.\n",
      "Adobe Inc.\n",
      "Novartis AG\n",
      "McDonald's Corporation\n",
      "Cisco Systems, Inc.\n",
      "Advanced Micro Devices, Inc.\n",
      "Salesforce, Inc.\n",
      "Shell plc\n",
      "Thermo Fisher Scientific Inc.\n",
      "Accenture plc\n",
      "Fomento Económico Mexicano, SAB de CV\n",
      "Netflix, Inc.\n",
      "Linde plc\n",
      "Abbott Laboratories\n",
      "Danaher Corporation\n",
      "NIKE, Inc.\n",
      "Comcast Corporation\n",
      "The Walt Disney Company\n",
      "Texas Instruments Incorporated\n",
      "SAP SE\n",
      "Wells Fargo & Company\n",
      "HSBC Holdings plc\n",
      "T-Mobile US, Inc.\n",
      "Verizon Communications Inc.\n",
      "United Parcel Service, Inc.\n",
      "NextEra Energy, Inc.\n",
      "Intel Corporation\n",
      "Morgan Stanley\n",
      "Philip Morris International Inc.\n",
      "Raytheon Technologies Corporation\n",
      "TotalEnergies SE\n",
      "QUALCOMM Incorporated\n",
      "Bristol-Myers Squibb Company\n",
      "Honeywell International Inc.\n",
      "The Boeing Company\n",
      "Royal Bank of Canada\n",
      "American Express Company\n",
      "Sanofi\n",
      "Unilever PLC\n",
      "S&P Global Inc.\n",
      "Lowe's Companies, Inc.\n",
      "Caterpillar Inc.\n",
      "Intuit Inc.\n",
      "International Business Machines Corporation\n",
      "ConocoPhillips\n",
      "Union Pacific Corporation\n",
      "Sony Group Corporation\n",
      "HDFC Bank Limited\n",
      "Amgen Inc.\n",
      "Applied Materials, Inc.\n",
      "Deere & Company\n",
      "Medtronic plc\n",
      "ServiceNow, Inc.\n",
      "Starbucks Corporation\n",
      "Lockheed Martin Corporation\n",
      "General Electric Company\n",
      "AT&T Inc.\n",
      "Intuitive Surgical, Inc.\n",
      "Prologis, Inc.\n",
      "The Goldman Sachs Group, Inc.\n",
      "Anheuser-Busch InBev SA/NV\n",
      "Stryker Corporation\n",
      "The Toronto-Dominion Bank\n",
      "Rio Tinto Group\n",
      "Blackstone Inc.\n",
      "PDD Holdings Inc.\n",
      "BlackRock, Inc.\n",
      "Elevance Health Inc.\n",
      "BP p.l.c.\n",
      "Mondelez International, Inc.\n",
      "Gilead Sciences, Inc.\n",
      "Diageo plc\n",
      "Booking Holdings Inc.\n",
      "Analog Devices, Inc.\n",
      "The Charles Schwab Corporation\n",
      "Citigroup Inc.\n",
      "Equinor ASA\n",
      "The TJX Companies, Inc.\n",
      "Petróleo Brasileiro S.A. - Petrobras\n",
      "American Tower Corporation\n",
      "Automatic Data Processing, Inc.\n",
      "Marsh & McLennan Companies, Inc.\n",
      "Vertex Pharmaceuticals Incorporated\n",
      "CVS Health Corporation\n",
      "Lam Research Corporation\n",
      "Shopify Inc.\n",
      "Mitsubishi UFJ Financial Group, Inc.\n",
      "Uber Technologies, Inc.\n",
      "Regeneron Pharmaceuticals, Inc.\n",
      "Petróleo Brasileiro S.A. - Petrobras\n",
      "ICICI Bank Limited\n",
      "Airbnb, Inc.\n",
      "Altria Group, Inc.\n",
      "Chubb Limited\n",
      "HCA Healthcare, Inc.\n",
      "Cigna Corporation\n",
      "Canadian National Railway Company\n",
      "Eaton Corporation plc\n",
      "The Southern Company\n",
      "Boston Scientific Corporation\n",
      "Zoetis Inc.\n",
      "Micron Technology, Inc.\n",
      "Enbridge Inc.\n",
      "Illinois Tool Works Inc.\n",
      "The Progressive Corporation\n",
      "Palo Alto Networks, Inc.\n",
      "Equinix, Inc.\n",
      "Canadian Pacific Railway Limited\n",
      "British American Tobacco p.l.c.\n",
      "Becton, Dickinson and Company\n",
      "PayPal Holdings, Inc.\n",
      "América Móvil, SAB de CV\n",
      "GSK plc\n",
      "Fiserv, Inc.\n",
      "Duke Energy Corporation\n",
      "The Estée Lauder Companies Inc.\n",
      "Northrop Grumman Corporation\n",
      "Synopsys, Inc.\n",
      "Schlumberger Limited\n",
      "CSX Corporation\n",
      "Waste Management, Inc.\n",
      "Aon plc\n",
      "KLA Corporation\n",
      "KKR & Co. Inc.\n",
      "CME Group Inc.\n",
      "EOG Resources, Inc.\n",
      "Infosys Limited\n",
      "Cadence Design Systems, Inc.\n",
      "Air Products and Chemicals, Inc.\n",
      "Activision Blizzard, Inc.\n",
      "UBS Group AG\n",
      "NetEase, Inc.\n",
      "The Sherwin-Williams Company\n",
      "Bank of Montreal\n",
      "Colgate-Palmolive Company\n",
      "Moody's Corporation\n",
      "Vale S.A.\n",
      "Intercontinental Exchange, Inc.\n",
      "RELX PLC\n",
      "Target Corporation\n",
      "Monster Beverage Corporation\n",
      "MercadoLibre, Inc.\n",
      "VMware, Inc.\n",
      "Canadian Natural Resources Limited\n",
      "JD.com, Inc.\n",
      "Thomson Reuters Corporation\n",
      "The Bank of Nova Scotia\n",
      "Snowflake Inc.\n",
      "General Dynamics Corporation\n",
      "FedEx Corporation\n",
      "Freeport-McMoRan Inc.\n",
      "Southern Copper Corporation\n",
      "Chipotle Mexican Grill, Inc.\n",
      "Humana Inc.\n",
      "Itaú Unibanco Holding S.A.\n",
      "Banco Santander, S.A.\n",
      "Workday, Inc.\n",
      "Enterprise Products Partners L.P.\n",
      "3M Company\n",
      "Fortinet, Inc.\n",
      "Ford Motor Company\n",
      "Sumitomo Mitsui Financial Group, Inc.\n",
      "O'Reilly Automotive, Inc.\n",
      "Marriott International, Inc.\n",
      "Ferrari N.V.\n",
      "Edwards Lifesciences Corporation\n",
      "Marvell Technology, Inc.\n",
      "Honda Motor Co., Ltd.\n",
      "The Hershey Company\n",
      "Stellantis N.V.\n",
      "McKesson Corporation\n",
      "Occidental Petroleum Corporation\n",
      "General Motors Company\n",
      "Brookfield Corporation\n",
      "Arista Networks, Inc.\n",
      "Baidu, Inc.\n",
      "Ecolab Inc.\n",
      "NXP Semiconductors N.V.\n",
      "The PNC Financial Services Group, Inc.\n",
      "Charter Communications, Inc.\n",
      "Norfolk Southern Corporation\n",
      "Takeda Pharmaceutical Company Limited\n",
      "Public Storage\n",
      "Crown Castle Inc.\n",
      "Ambev S.A.\n",
      "Cintas Corporation\n",
      "DexCom, Inc.\n",
      "U.S. Bancorp\n",
      "Emerson Electric Co.\n",
      "Roper Technologies, Inc.\n",
      "National Grid plc\n",
      "Kenvue Inc.\n",
      "Amphenol Corporation\n",
      "Moderna, Inc.\n",
      "Marathon Petroleum Corporation\n",
      "Lululemon Athletica Inc.\n",
      "ING Groep N.V.\n",
      "General Mills, Inc.\n",
      "Microchip Technology Incorporated\n",
      "Parker-Hannifin Corporation\n",
      "Pioneer Natural Resources Company\n",
      "Eni S.p.A.\n",
      "Motorola Solutions, Inc.\n",
      "Atlassian Corporation Plc\n",
      "Sempra\n",
      "Banco Santander (Brasil) S.A.\n",
      "Las Vegas Sands Corp.\n",
      "Autodesk, Inc.\n",
      "Kimberly-Clark Corporation\n",
      "Republic Services, Inc.\n",
      "Constellation Brands, Inc.\n",
      "STMicroelectronics N.V.\n",
      "The Kraft Heinz Company\n",
      "Arthur J. Gallagher & Co.\n",
      "Apollo Global Management, Inc.\n",
      "Keurig Dr Pepper Inc.\n",
      "TransDigm Group Incorporated\n",
      "Woodside Energy Group Ltd\n",
      "Johnson Controls International plc\n",
      "Dominion Energy, Inc.\n",
      "Phillips 66\n",
      "AutoZone, Inc.\n",
      "Banco Bilbao Vizcaya Argentaria, S.A.\n",
      "Biogen Inc.\n",
      "American Electric Power Company, Inc.\n",
      "Capital One Financial Corporation\n",
      "PG&E Corporation\n",
      "Truist Financial Corporation\n",
      "Aflac Incorporated\n",
      "TE Connectivity Ltd.\n",
      "Simon Property Group, Inc.\n",
      "Trane Technologies plc\n",
      "MetLife, Inc.\n",
      "BCE Inc.\n",
      "Copart, Inc.\n",
      "Realty Income Corporation\n",
      "Hess Corporation\n",
      "TC Energy Corporation\n",
      "PACCAR Inc\n",
      "Corteva, Inc.\n",
      "Welltower Inc.\n",
      "Valero Energy Corporation\n",
      "American International Group, Inc.\n",
      "Paychex, Inc.\n",
      "The Travelers Companies, Inc.\n",
      "ON Semiconductor Corporation\n",
      "Exelon Corporation\n",
      "Archer-Daniels-Midland Company\n",
      "Energy Transfer LP\n",
      "Canadian Imperial Bank of Commerce\n",
      "Prudential plc\n",
      "IQVIA Holdings Inc.\n",
      "Alcon Inc.\n",
      "Suncor Energy Inc.\n",
      "MSCI Inc.\n",
      "D.R. Horton, Inc.\n",
      "IDEXX Laboratories, Inc.\n",
      "Carrier Global Corporation\n",
      "Block, Inc.\n",
      "Mizuho Financial Group, Inc.\n",
      "Yum! Brands, Inc.\n",
      "CRH plc\n",
      "Kinder Morgan, Inc.\n",
      "Hilton Worldwide Holdings Inc.\n",
      "Haleon plc\n",
      "Nucor Corporation\n",
      "The Trade Desk, Inc.\n",
      "Dow Inc.\n",
      "Banco Bradesco S.A.\n",
      "Sysco Corporation\n",
      "Lloyds Banking Group plc\n",
      "Seagen Inc.\n",
      "The Williams Companies, Inc.\n",
      "CrowdStrike Holdings, Inc.\n",
      "W.W. Grainger, Inc.\n",
      "AmerisourceBergen Corporation\n",
      "Sea Limited\n",
      "Ross Stores, Inc.\n",
      "L3Harris Technologies, Inc.\n",
      "Otis Worldwide Corporation\n",
      "Rockwell Automation, Inc.\n",
      "Dell Technologies Inc.\n",
      "Cheniere Energy, Inc.\n",
      "GE HealthCare Technologies Inc.\n",
      "Dollar General Corporation\n",
      "Centene Corporation\n",
      "AMETEK, Inc.\n",
      "Manulife Financial Corporation\n",
      "Agilent Technologies, Inc.\n",
      "Waste Connections, Inc.\n",
      "Old Dominion Freight Line, Inc.\n",
      "Electronic Arts Inc.\n",
      "The Bank of New York Mellon Corporation\n",
      "Restaurant Brands International Inc.\n",
      "Nu Holdings Ltd.\n",
      "GLOBALFOUNDRIES Inc.\n",
      "Xcel Energy Inc.\n",
      "The Kroger Co.\n",
      "CoStar Group, Inc.\n",
      "MPLX LP\n",
      "Palantir Technologies Inc.\n",
      "Ameriprise Financial, Inc.\n",
      "Newmont Corporation\n",
      "Warner Bros. Discovery, Inc.\n",
      "Interactive Brokers Group, Inc.\n",
      "Cummins Inc.\n",
      "PPG Industries, Inc.\n",
      "Li Auto Inc.\n",
      "Mobileye Global Inc.\n",
      "Lennar Corporation\n",
      "VICI Properties Inc.\n",
      "Fidelity National Information Services, Inc.\n",
      "Illumina, Inc.\n",
      "Cognizant Technology Solutions Corporation\n",
      "DuPont de Nemours, Inc.\n",
      "Consolidated Edison, Inc.\n",
      "Verisk Analytics, Inc.\n",
      "ResMed Inc.\n",
      "Chunghwa Telecom Co., Ltd.\n",
      "Cenovus Energy Inc.\n",
      "Brown-Forman Corporation\n",
      "Fastenal Company\n",
      "Devon Energy Corporation\n",
      "Prudential Financial, Inc.\n",
      "Veeva Systems Inc.\n",
      "Datadog, Inc.\n",
      "Barclays PLC\n",
      "Public Service Enterprise Group Incorporated\n",
      "Digital Realty Trust, Inc.\n",
      "Ferguson plc\n",
      "HP Inc.\n",
      "Dollar Tree, Inc.\n",
      "Baker Hughes Company\n",
      "Constellation Energy Corporation\n",
      "NatWest Group plc\n",
      "Coca-Cola Europacific Partners PLC\n",
      "Coupang, Inc.\n",
      "Halliburton Company\n",
      "Zimmer Biomet Holdings, Inc.\n",
      "Discover Financial Services\n",
      "Orange S.A.\n",
      "Sun Life Financial Inc.\n",
      "Barrick Gold Corporation\n",
      "LyondellBasell Industries N.V.\n",
      "Spotify Technology S.A.\n",
      "Keysight Technologies, Inc.\n",
      "The Allstate Corporation\n",
      "Imperial Oil Limited\n",
      "Nutrien Ltd.\n",
      "ANSYS, Inc.\n",
      "Mettler-Toledo International Inc.\n",
      "WEC Energy Group, Inc.\n",
      "Ares Management Corporation\n",
      "American Water Works Company, Inc.\n",
      "Corning Incorporated\n",
      "Equifax Inc.\n",
      "United Rentals, Inc.\n",
      "Gartner, Inc.\n",
      "DoorDash, Inc.\n",
      "Franco-Nevada Corporation\n",
      "TELUS Corporation\n",
      "Vulcan Materials Company\n",
      "Aptiv PLC\n",
      "Walgreens Boots Alliance, Inc.\n",
      "Delta Air Lines, Inc.\n",
      "ONEOK, Inc.\n",
      "AvalonBay Communities, Inc.\n",
      "Quanta Services, Inc.\n",
      "Xylem Inc.\n",
      "Symbotic Inc.\n",
      "BioNTech SE\n",
      "Albemarle Corporation\n",
      "Perusahaan Perseroan (Persero) PT Telekomunikasi Indonesia Tbk\n",
      "Global Payments Inc.\n",
      "MongoDB, Inc.\n",
      "Martin Marietta Materials, Inc.\n",
      "West Pharmaceutical Services, Inc.\n",
      "Edison International\n",
      "Arch Capital Group Ltd.\n",
      "Ingersoll Rand Inc.\n",
      "Wipro Limited\n",
      "Monolithic Power Systems, Inc.\n",
      "Equity Residential\n",
      "Nasdaq, Inc.\n",
      "Genmab A/S\n",
      "HubSpot, Inc.\n",
      "T. Rowe Price Group, Inc.\n",
      "Vodafone Group PLC\n",
      "SBA Communications Corporation\n",
      "Agnico Eagle Mines Limited\n",
      "Alnylam Pharmaceuticals, Inc.\n",
      "Yum China Holdings, Inc.\n",
      "Align Technology, Inc.\n",
      "CGI Inc.\n",
      "Enphase Energy, Inc.\n",
      "State Street Corporation\n",
      "Fortive Corporation\n",
      "Ryanair Holdings plc\n",
      "McCormick & Company, Incorporated\n",
      "Eversource Energy\n",
      "Roblox Corporation\n",
      "CBRE Group, Inc.\n",
      "Trip.com Group Limited\n",
      "Willis Towers Watson PLC\n",
      "Royal Caribbean Cruises Ltd.\n",
      "eBay Inc.\n",
      "Tractor Supply Company\n",
      "CDW Corporation\n",
      "UDR, Inc.\n",
      "ArcelorMittal S.A.\n",
      "POSCO Holdings Inc.\n",
      "Rogers Communications Inc.\n",
      "Church & Dwight Co., Inc.\n",
      "DTE Energy Company\n",
      "Take-Two Interactive Software, Inc.\n",
      "Nokia Oyj\n",
      "Horizon Therapeutics PLC\n",
      "Zscaler, Inc.\n",
      "VeriSign, Inc.\n",
      "Diamondback Energy, Inc.\n",
      "Cloudflare, Inc.\n",
      "Kellogg Company\n",
      "ZTO Express (Cayman) Inc.\n",
      "Telefónica, S.A.\n",
      "Hormel Foods Corporation\n",
      "Weyerhaeuser Company\n",
      "Genuine Parts Company\n",
      "FirstEnergy Corp.\n",
      "United Microelectronics Corporation\n",
      "Cardinal Health, Inc.\n",
      "Ulta Beauty, Inc.\n",
      "Companhia Paranaense de Energia - COPEL\n",
      "Teck Resources Limited\n",
      "Baxter International Inc.\n",
      "Cheniere Energy Partners, L.P.\n",
      "The Hartford Financial Services Group, Inc.\n",
      "argenx SE\n",
      "BeiGene, Ltd.\n",
      "Deutsche Bank Aktiengesellschaft\n",
      "Ameren Corporation\n",
      "Hewlett Packard Enterprise Company\n",
      "KE Holdings Inc.\n",
      "Entergy Corporation\n",
      "Alexandria Real Estate Equities, Inc.\n",
      "Ecopetrol S.A.\n",
      "STERIS plc\n",
      "Invitation Homes Inc.\n",
      "Raymond James Financial, Inc.\n",
      "First Solar, Inc.\n",
      "Fortis Inc.\n",
      "Rentokil Initial plc\n",
      "Extra Space Storage Inc.\n",
      "Garmin Ltd.\n",
      "Zoom Video Communications, Inc.\n",
      "Meta Data Limited\n",
      "Carnival Corporation & plc\n",
      "Dover Corporation\n",
      "Darden Restaurants, Inc.\n",
      "Sociedad Química y Minera de Chile S.A.\n",
      "M&T Bank Corporation\n",
      "Wheaton Precious Metals Corp.\n",
      "Rollins, Inc.\n",
      "Laboratory Corporation of America Holdings\n",
      "ORIX Corporation\n",
      "Live Nation Entertainment, Inc.\n",
      "Southwest Airlines Co.\n"
     ]
    }
   ],
   "source": [
    "def get_company_names():\n",
    "    url = \"https://stockanalysis.com/list/biggest-companies/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    names = []\n",
    "\n",
    "    # Find all the table cells with the specified class\n",
    "    cells = soup.find_all(\"td\", class_=\"slw svelte-1tv1ofl\")\n",
    "    for cell in cells:\n",
    "        # Extract the text content of each cell (company name)\n",
    "        name = cell.text.strip()\n",
    "        names.append(name)\n",
    "\n",
    "    return names[:1000]\n",
    "\n",
    "# Get the list of company names\n",
    "company_names = get_company_names()\n",
    "\n",
    "# Print the company names\n",
    "for name in company_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e86e7c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a company name: Apple\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 429: Too Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 61>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m company_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a company name: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Count the positive and negative news sentiments for the company\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m positive_count, negative_count \u001b[38;5;241m=\u001b[39m \u001b[43mcount_sentiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositive News Count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpositive_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36mcount_sentiments\u001b[1;34m(company_name)\u001b[0m\n\u001b[0;32m     38\u001b[0m negative_news_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Iterate through the search results and process the articles\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m search_results:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Check if the URL is an article link\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/article/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m url:\n\u001b[0;32m     44\u001b[0m         keyword_counts \u001b[38;5;241m=\u001b[39m get_article_keywords(url)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:305\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(query, tld, lang, tbs, safe, num, start, stop, pause, country, extra_params, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    302\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(pause)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# Request the Google Search results page.\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mget_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_ssl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Parse the response and get every anchored URL.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bs4:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:174\u001b[0m, in \u001b[0;36mget_page\u001b[1;34m(url, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    172\u001b[0m cookie_jar\u001b[38;5;241m.\u001b[39madd_cookie_header(request)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_ssl:\n\u001b[1;32m--> 174\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     context \u001b[38;5;241m=\u001b[39m ssl\u001b[38;5;241m.\u001b[39m_create_unverified_context()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:555\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    553\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    554\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[1;32m--> 555\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:747\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    744\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    745\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 429: Too Many Requests"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search as google_search\n",
    "\n",
    "def get_article_keywords(url):\n",
    "    # Define the keywords related to positive news and negative news\n",
    "    positive_keywords = ['positive', 'improvement', 'growth', 'increase']\n",
    "    negative_keywords = ['negative', 'decline', 'loss', 'decrease']\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the article URL\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content of the article\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get the text content of the article\n",
    "        article_text = soup.get_text().lower()\n",
    "\n",
    "        # Count the occurrences of positive and negative news keywords\n",
    "        positive_count = sum(article_text.count(keyword) for keyword in positive_keywords)\n",
    "        negative_count = sum(article_text.count(keyword) for keyword in negative_keywords)\n",
    "\n",
    "        return positive_count, negative_count\n",
    "\n",
    "    except (requests.RequestException, ValueError, AttributeError) as e:\n",
    "        print(f\"Error accessing article: {url}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def count_sentiments(company_name):\n",
    "    # Perform a Google search for the company name\n",
    "    search_results = google_search(company_name, num=10, stop=10, pause=2.0)\n",
    "\n",
    "    positive_news_count = 0\n",
    "    negative_news_count = 0\n",
    "\n",
    "    # Iterate through the search results and process the articles\n",
    "    for url in search_results:\n",
    "        # Check if the URL is an article link\n",
    "        if '/article/' in url:\n",
    "            keyword_counts = get_article_keywords(url)\n",
    "\n",
    "            if keyword_counts is None:\n",
    "                continue\n",
    "\n",
    "            positive_count, negative_count = keyword_counts\n",
    "\n",
    "            # Update the overall counts\n",
    "            positive_news_count += positive_count\n",
    "            negative_news_count += negative_count\n",
    "\n",
    "    return positive_news_count, negative_news_count\n",
    "\n",
    "# Get the company name as input\n",
    "company_name = input(\"Enter a company name: \")\n",
    "\n",
    "# Count the positive and negative news sentiments for the company\n",
    "positive_count, negative_count = count_sentiments(company_name)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Positive News Count: {positive_count}\")\n",
    "print(f\"Negative News Count: {negative_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f9cdecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a company name: apple\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "search() got an unexpected keyword argument 'num_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m company_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a company name: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Retrieve the URLs of the first 10 search results for the company\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m search_result_urls \u001b[38;5;241m=\u001b[39m \u001b[43mget_search_results_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Print the URLs\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m search_result_urls:\n",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36mget_search_results_urls\u001b[1;34m(query, num_results)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_search_results_urls\u001b[39m(query, num_results):\n\u001b[1;32m----> 4\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(search_results)\n",
      "\u001b[1;31mTypeError\u001b[0m: search() got an unexpected keyword argument 'num_results'"
     ]
    }
   ],
   "source": [
    "from googlesearch import search as google_search\n",
    "\n",
    "def get_search_results_urls(query, num_results):\n",
    "    search_results = google_search(query, num_results=num_results)\n",
    "    return list(search_results)\n",
    "\n",
    "# Get the company name as input\n",
    "company_name = input(\"Enter a company name: \")\n",
    "\n",
    "# Retrieve the URLs of the first 10 search results for the company\n",
    "search_result_urls = get_search_results_urls(company_name, num_results=10)\n",
    "\n",
    "# Print the URLs\n",
    "for url in search_result_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5722d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a company name: apple\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "search() got an unexpected keyword argument 'num_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m company_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a company name: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Retrieve the URLs of the first 10 search results for the company\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m search_result_urls \u001b[38;5;241m=\u001b[39m \u001b[43mget_search_results_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Print the URLs\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m search_result_urls:\n",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36mget_search_results_urls\u001b[1;34m(query, num_results)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_search_results_urls\u001b[39m(query, num_results):\n\u001b[1;32m----> 4\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(search_results)\n",
      "\u001b[1;31mTypeError\u001b[0m: search() got an unexpected keyword argument 'num_results'"
     ]
    }
   ],
   "source": [
    "from googlesearch import search as google_search\n",
    "\n",
    "def get_search_results_urls(query, num_results):\n",
    "    search_results = google_search(query, num_results=num_results, stop=num_results)\n",
    "    return list(search_results)\n",
    "\n",
    "# Get the company name as input\n",
    "company_name = input(\"Enter a company name: \")\n",
    "\n",
    "# Retrieve the URLs of the first 10 search results for the company\n",
    "search_result_urls = get_search_results_urls(company_name, num_results=10)\n",
    "\n",
    "# Print the URLs\n",
    "for url in search_result_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9796e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a company name: Apple\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "search() got an unexpected keyword argument 'num_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m company_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a company name: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Retrieve the URLs of the first 10 search results for the company\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m search_result_urls \u001b[38;5;241m=\u001b[39m \u001b[43mget_search_results_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Print the URLs\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m search_result_urls:\n",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36mget_search_results_urls\u001b[1;34m(query, num_results)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_search_results_urls\u001b[39m(query, num_results):\n\u001b[1;32m----> 4\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(search_results)\n",
      "\u001b[1;31mTypeError\u001b[0m: search() got an unexpected keyword argument 'num_results'"
     ]
    }
   ],
   "source": [
    "from googlesearch import search as google_search\n",
    "\n",
    "def get_search_results_urls(query, num_results):\n",
    "    search_results = google_search(query, num_results=num_results, stop=num_results)\n",
    "    return list(search_results)\n",
    "\n",
    "# Get the company name as input\n",
    "company_name = input(\"Enter a company name: \")\n",
    "\n",
    "# Retrieve the URLs of the first 10 search results for the company\n",
    "search_result_urls = get_search_results_urls(company_name, num_results=10)\n",
    "\n",
    "# Print the URLs\n",
    "for url in search_result_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89866a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a company name: apple\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "search() got an unexpected keyword argument 'num_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m company_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a company name: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Retrieve the URLs of the first 10 search results for the company\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m search_result_urls \u001b[38;5;241m=\u001b[39m \u001b[43mget_search_results_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Print the URLs\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m search_result_urls:\n",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36mget_search_results_urls\u001b[1;34m(query, num_results)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_search_results_urls\u001b[39m(query, num_results):\n\u001b[1;32m----> 4\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(search_results)\n",
      "\u001b[1;31mTypeError\u001b[0m: search() got an unexpected keyword argument 'num_results'"
     ]
    }
   ],
   "source": [
    "from googlesearch import search as google_search\n",
    "\n",
    "def get_search_results_urls(query, num_results):\n",
    "    search_results = google_search(query, num_results=num_results, stop=num_results)\n",
    "    return list(search_results)\n",
    "\n",
    "# Get the company name as input\n",
    "company_name = input(\"Enter a company name: \")\n",
    "\n",
    "# Retrieve the URLs of the first 10 search results for the company\n",
    "search_result_urls = get_search_results_urls(company_name, num_results=10)\n",
    "\n",
    "# Print the URLs\n",
    "for url in search_result_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a3c1288",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "search() got an unexpected keyword argument 'num_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApple\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Retrieve the URLs of the first 10 search results\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m search_result_urls \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Print the URLs\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m search_result_urls:\n",
      "\u001b[1;31mTypeError\u001b[0m: search() got an unexpected keyword argument 'num_results'"
     ]
    }
   ],
   "source": [
    "from googlesearch import search as google_search\n",
    "\n",
    "# Define the query\n",
    "query = \"Apple\"\n",
    "\n",
    "# Retrieve the URLs of the first 10 search results\n",
    "search_result_urls = google_search(query, num_results=10, stop=10)\n",
    "\n",
    "# Print the URLs\n",
    "for url in search_result_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1dddd25d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "search() got an unexpected keyword argument 'num_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m num_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      8\u001b[0m search_result_urls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgoogle_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_results\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     10\u001b[0m     search_result_urls\u001b[38;5;241m.\u001b[39mappend(url)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Print the URLs\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: search() got an unexpected keyword argument 'num_results'"
     ]
    }
   ],
   "source": [
    "from googlesearch import search as google_search\n",
    "\n",
    "# Define the query\n",
    "query = \"Apple\"\n",
    "\n",
    "# Retrieve the URLs of the first 10 search results\n",
    "num_results = 10\n",
    "search_result_urls = []\n",
    "for url in google_search(query, num_results=num_results, stop=num_results):\n",
    "    search_result_urls.append(url)\n",
    "\n",
    "# Print the URLs\n",
    "for url in search_result_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d84798a",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 429: Too Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m num_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      8\u001b[0m search_result_urls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m google_search(query, stop\u001b[38;5;241m=\u001b[39mnum_results):\n\u001b[0;32m     10\u001b[0m     search_result_urls\u001b[38;5;241m.\u001b[39mappend(url)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(search_result_urls) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m num_results:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:305\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(query, tld, lang, tbs, safe, num, start, stop, pause, country, extra_params, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    302\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(pause)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# Request the Google Search results page.\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mget_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_ssl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Parse the response and get every anchored URL.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bs4:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googlesearch\\__init__.py:174\u001b[0m, in \u001b[0;36mget_page\u001b[1;34m(url, user_agent, verify_ssl)\u001b[0m\n\u001b[0;32m    172\u001b[0m cookie_jar\u001b[38;5;241m.\u001b[39madd_cookie_header(request)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_ssl:\n\u001b[1;32m--> 174\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     context \u001b[38;5;241m=\u001b[39m ssl\u001b[38;5;241m.\u001b[39m_create_unverified_context()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:555\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    553\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    554\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[1;32m--> 555\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:747\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    744\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    745\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 429: Too Many Requests"
     ]
    }
   ],
   "source": [
    "from googlesearch import search as google_search\n",
    "\n",
    "# Define the query\n",
    "query = \"Apple\"\n",
    "\n",
    "# Retrieve the URLs of the first 10 search results\n",
    "num_results = 10\n",
    "search_result_urls = []\n",
    "for url in google_search(query, stop=num_results):\n",
    "    search_result_urls.append(url)\n",
    "    if len(search_result_urls) >= num_results:\n",
    "        break\n",
    "\n",
    "# Print the URLs\n",
    "for url in search_result_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c05d26fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HTML' from 'requests' (C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\site-packages\\requests\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTMLSession\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_google\u001b[39m(query):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'HTML' from 'requests' (C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\site-packages\\requests\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from requests import HTML\n",
    "from requests import HTMLSession\n",
    "\n",
    "def scrape_google(query):\n",
    "\n",
    "    query = urllib.parse.quote_plus(query)\n",
    "    response = get_source(\"https://www.google.com/search?q=\" + query)\n",
    "\n",
    "    links = list(response.html.absolute_links)\n",
    "    google_domains = ('https://www.google.', \n",
    "                      'https://google.', \n",
    "                      'https://webcache.googleusercontent.', \n",
    "                      'http://webcache.googleusercontent.', \n",
    "                      'https://policies.google.',\n",
    "                      'https://support.google.',\n",
    "                      'https://maps.google.')\n",
    "\n",
    "    for url in links[:]:\n",
    "        if url.startswith(google_domains):\n",
    "            links.remove(url)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca28f6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests_html\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: w3lib in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from requests_html) (1.21.0)\n",
      "Collecting fake-useragent\n",
      "  Downloading fake_useragent-1.1.3-py3-none-any.whl (50 kB)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Collecting pyppeteer>=0.0.14\n",
      "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
      "Collecting pyquery\n",
      "  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting parse\n",
      "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from requests_html) (2.27.1)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n",
      "Requirement already satisfied: certifi>=2021 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html) (2021.10.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html) (4.64.0)\n",
      "Collecting pyee<9.0.0,>=8.1.0\n",
      "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
      "Collecting websockets<11.0,>=10.0\n",
      "  Downloading websockets-10.4-cp39-cp39-win_amd64.whl (101 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html) (4.11.3)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests_html) (1.26.9)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html) (3.7.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests_html) (0.4.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from bs4->requests_html) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4->requests_html) (2.3.1)\n",
      "Collecting importlib-resources>=5.0\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: lxml>=2.1 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from pyquery->requests_html) (4.9.2)\n",
      "Collecting cssselect>=1.2.0\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from requests->requests_html) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from requests->requests_html) (3.3)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from w3lib->requests_html) (1.16.0)\n",
      "Building wheels for collected packages: bs4, parse\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=1779a73e16ba599c786b8b559e9032858f3f82fa627de5b094bb2a74fd7db7ac\n",
      "  Stored in directory: c:\\users\\michaelpellegrino\\appdata\\local\\pip\\cache\\wheels\\73\\2b\\cb\\099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
      "  Building wheel for parse (setup.py): started\n",
      "  Building wheel for parse (setup.py): finished with status 'done'\n",
      "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24591 sha256=cbb1b0f362c3bad3e1c651749d8569e1f32dc63b71351551a9e752c614699e7d\n",
      "  Stored in directory: c:\\users\\michaelpellegrino\\appdata\\local\\pip\\cache\\wheels\\d6\\9c\\58\\ee3ba36897e890f3ad81e9b730791a153fce20caa4a8a474df\n",
      "Successfully built bs4 parse\n",
      "Installing collected packages: websockets, pyee, importlib-resources, cssselect, pyquery, pyppeteer, parse, fake-useragent, bs4, requests-html\n",
      "  Attempting uninstall: cssselect\n",
      "    Found existing installation: cssselect 1.1.0\n",
      "    Uninstalling cssselect-1.1.0:\n",
      "      Successfully uninstalled cssselect-1.1.0\n",
      "Successfully installed bs4-0.0.1 cssselect-1.2.0 fake-useragent-1.1.3 importlib-resources-5.12.0 parse-1.19.0 pyee-8.2.2 pyppeteer-1.0.2 pyquery-2.0.0 requests-html-0.10.0 websockets-10.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "debe3765",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests_HTML'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests_HTML\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests_HTML\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTMLSession\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_google\u001b[39m(query):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests_HTML'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from requests_HTML import HTML\n",
    "from requests_HTML import HTMLSession\n",
    "\n",
    "def scrape_google(query):\n",
    "\n",
    "    query = urllib.parse.quote_plus(query)\n",
    "    response = get_source(\"https://www.google.com/search?q=\" + query)\n",
    "\n",
    "    links = list(response.html.absolute_links)\n",
    "    google_domains = ('https://www.google.', \n",
    "                      'https://google.', \n",
    "                      'https://webcache.googleusercontent.', \n",
    "                      'http://webcache.googleusercontent.', \n",
    "                      'https://policies.google.',\n",
    "                      'https://support.google.',\n",
    "                      'https://maps.google.')\n",
    "\n",
    "    for url in links[:]:\n",
    "        if url.startswith(google_domains):\n",
    "            links.remove(url)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21be051d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     response \u001b[38;5;241m=\u001b[39m get_results(query)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_results(response)\n\u001b[1;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb scraping\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m results\n",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36mgoogle_search\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgoogle_search\u001b[39m(query):\n\u001b[1;32m----> 2\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mget_results\u001b[49m(query)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_results(response)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_results' is not defined"
     ]
    }
   ],
   "source": [
    "def google_search(query):\n",
    "    response = get_results(query)\n",
    "    return parse_results(response)\n",
    "results = google_search(\"web scraping\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd84371c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "search() got an unexpected keyword argument 'num_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m num_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      8\u001b[0m search_result_urls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m \u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_results\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     10\u001b[0m     search_result_urls\u001b[38;5;241m.\u001b[39mappend(url)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Print the URLs\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: search() got an unexpected keyword argument 'num_results'"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "\n",
    "# Define the query\n",
    "query = \"Microsoft\"\n",
    "\n",
    "# Retrieve the URLs of the first 10 search results\n",
    "num_results = 10\n",
    "search_result_urls = []\n",
    "for url in search(query, num_results=num_results, stop=num_results):\n",
    "    search_result_urls.append(url)\n",
    "\n",
    "# Print the URLs\n",
    "for url in search_result_urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed8467b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.google.com/search?q=apple&tbm=nws\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the search result links\n",
    "search_results = soup.find_all(\"a\", class_=\"dbsr\")\n",
    "\n",
    "# Retrieve the URLs of the first 10 search results\n",
    "num_results = 10\n",
    "search_result_urls = [result[\"href\"] for result in search_results[:num_results]]\n",
    "\n",
    "# Print the URLs\n",
    "for url in search_result_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15178467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.google.com/search?q=apple&tbm=nws\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the search result elements\n",
    "search_results = soup.find_all(\"div\", class_=\"n0jPhd ynAwRc MBeuO nDgy9d\")\n",
    "\n",
    "# Retrieve the titles of the first 10 search results\n",
    "num_results = 10\n",
    "search_result_titles = [result.get_text() for result in search_results[:num_results]]\n",
    "\n",
    "# Print the titles\n",
    "for title in search_result_titles:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc9f3d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search query: apple\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Take user input for the search query\n",
    "query = input(\"Enter a search query: \")\n",
    "\n",
    "# Construct the Google search URL\n",
    "google_url = f\"https://www.google.com/search?q={query}&tbm=nws\"\n",
    "\n",
    "# Send a GET request to the Google search URL\n",
    "response = requests.get(google_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Extract the HTML content from the response\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Find the URLs of the top 10 results in the HTML content\n",
    "    start_marker = 'class=\"kCrYT\"><a href=\"/url?q='\n",
    "    end_marker = '&amp;'\n",
    "    result_urls = []\n",
    "    start_index = html_content.find(start_marker)\n",
    "    \n",
    "    while start_index != -1 and len(result_urls) < 10:\n",
    "        start_index += len(start_marker)\n",
    "        end_index = html_content.find(end_marker, start_index)\n",
    "        url = html_content[start_index:end_index]\n",
    "        result_urls.append(url)\n",
    "        start_index = html_content.find(start_marker, end_index)\n",
    "    \n",
    "    # Print the URLs of the top 10 results\n",
    "    for url in result_urls:\n",
    "        print(url)\n",
    "else:\n",
    "    print(\"Error: Unable to fetch search results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa67e054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search query: apple\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Take user input for the search query\n",
    "query = input(\"Enter a search query: \")\n",
    "\n",
    "# Construct the Google search URL\n",
    "google_url = f\"https://www.google.com/search?q={query}&tbm=nws\"\n",
    "\n",
    "# Send a GET request to the Google search URL\n",
    "response = requests.get(google_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Extract the HTML content from the response\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Create a BeautifulSoup object for parsing the HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find the div elements that contain the search results\n",
    "    result_divs = soup.find_all('div', class_='ZINbbc')\n",
    "    \n",
    "    # Extract the URLs of the top 10 results\n",
    "    result_urls = []\n",
    "    for div in result_divs:\n",
    "        link = div.find('a')\n",
    "        if link and link['href'].startswith('/url?q='):\n",
    "            url = link['href'][7:]\n",
    "            result_urls.append(url)\n",
    "            if len(result_urls) == 10:\n",
    "                break\n",
    "    \n",
    "    # Print the URLs of the top 10 results\n",
    "    for url in result_urls:\n",
    "        print(url)\n",
    "else:\n",
    "    print(\"Error: Unable to fetch search results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0637a190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object search at 0x0000027601FA2D60>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "search(\"Google\", num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3969798e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object search at 0x00000276039CD270>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "search(\"Google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3c817728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Queryapple\n",
      "https://www.apple.com/\n",
      "https://www.macrumors.com/2023/06/14/new-mac-pro-sata-drive-issue/\n",
      "https://www.cnet.com/tech/services-and-software/with-ios-17-apple-catches-up-to-rivals-in-messaging-wars/\n",
      "https://www.engadget.com/apples-new-153-inch-macbook-air-is-already-100-off-101050515.html\n",
      "https://en.wikipedia.org/wiki/Apple_Inc.\n",
      "http://www.apple.com/\n",
      "http://t1.gstatic.com/images?q=tbn:ANd9GcSjoU2lZ2eJX3aCMfiFDt39uRNcDu9W7pTKcyZymE2iKa7IOVaI\n",
      "https://finance.yahoo.com/quote/AAPL?p=AAPL\n",
      "https://twitter.com/apple?lang=en\n",
      "https://www.instagram.com/apple/\n"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "query = input(\"Enter Query\")\n",
    "\n",
    "for i in search(query, tld=\"com\", num=10, stop=10, pause=2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "339eed0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GoogleNews\n",
      "  Downloading GoogleNews-1.6.8-py3-none-any.whl (8.1 kB)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from GoogleNews) (2.8.2)\n",
      "Collecting dateparser\n",
      "  Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from GoogleNews) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from beautifulsoup4->GoogleNews) (2.3.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from dateparser->GoogleNews) (2023.3)\n",
      "Collecting tzlocal\n",
      "  Downloading tzlocal-5.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from dateparser->GoogleNews) (2022.3.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from python-dateutil->GoogleNews) (1.16.0)\n",
      "Collecting tzdata\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Installing collected packages: tzdata, tzlocal, dateparser, GoogleNews\n",
      "Successfully installed GoogleNews-1.6.8 dateparser-1.1.8 tzdata-2023.3 tzlocal-5.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install GoogleNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3cace35b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "search() missing 1 required positional argument: 'key'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mGoogleNews\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GoogleNews\n\u001b[1;32m----> 2\u001b[0m \u001b[43mGoogleNews\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mApple\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: search() missing 1 required positional argument: 'key'"
     ]
    }
   ],
   "source": [
    "from GoogleNews import GoogleNews\n",
    "GoogleNews.search('Apple')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1acf6cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygooglenews\n",
      "  Using cached pygooglenews-0.1.2-py3-none-any.whl (10 kB)\n",
      "Collecting feedparser<6.0.0,>=5.2.1\n",
      "  Using cached feedparser-5.2.1.zip (1.2 MB)\n",
      "  Using cached feedparser-5.2.1.tar.gz (252 kB)\n",
      "  Using cached feedparser-5.2.1.tar.bz2 (192 kB)\n",
      "Collecting pygooglenews\n",
      "  Using cached pygooglenews-0.1.1-py3-none-any.whl (10 kB)\n",
      "  Using cached pygooglenews-0.1.0-py3-none-any.whl (3.8 kB)\n",
      "\n",
      "The conflict is caused by:\n",
      "    pygooglenews 0.1.2 depends on feedparser<6.0.0 and >=5.2.1\n",
      "    pygooglenews 0.1.1 depends on feedparser<6.0.0 and >=5.2.1\n",
      "    pygooglenews 0.1.0 depends on feedparser<6.0.0 and >=5.2.1\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-75z8_ov9\\\\feedparser_a337082c7a914864904423c68b7e5f27\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-75z8_ov9\\\\feedparser_a337082c7a914864904423c68b7e5f27\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-xvt_ndrg'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-75z8_ov9\\feedparser_a337082c7a914864904423c68b7e5f27\\\n",
      "    Complete output (1 lines):\n",
      "    error in feedparser setup command: use_2to3 is invalid.\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/5d/20/c6ec54fc79fb944f2058a5fef149b8a0d67993db5e3ba7ea58ce931e365e/feedparser-5.2.1.zip#sha256=cd2485472e41471632ed3029d44033ee420ad0b57111db95c240c9160a85831c (from https://pypi.org/simple/feedparser/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-75z8_ov9\\\\feedparser_b9cb76af41ae42c2abfa2b602dd896cd\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-75z8_ov9\\\\feedparser_b9cb76af41ae42c2abfa2b602dd896cd\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-7eypyq_d'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-75z8_ov9\\feedparser_b9cb76af41ae42c2abfa2b602dd896cd\\\n",
      "    Complete output (1 lines):\n",
      "    error in feedparser setup command: use_2to3 is invalid.\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/ca/f4/91a056f11751701c24f86c692d92fee290b0ba3f99f657cdeb85ad3da402/feedparser-5.2.1.tar.gz#sha256=bd030652c2d08532c034c27fcd7c85868e7fa3cb2b17f230a44a6bbc92519bf9 (from https://pypi.org/simple/feedparser/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-75z8_ov9\\\\feedparser_ec4ce8833802413fb8e5455846ab74f9\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-75z8_ov9\\\\feedparser_ec4ce8833802413fb8e5455846ab74f9\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-4ldf97n5'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-75z8_ov9\\feedparser_ec4ce8833802413fb8e5455846ab74f9\\\n",
      "    Complete output (1 lines):\n",
      "    error in feedparser setup command: use_2to3 is invalid.\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2#sha256=ce875495c90ebd74b179855449040003a1beb40cd13d5f037a0654251e260b02 (from https://pypi.org/simple/feedparser/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "ERROR: Cannot install pygooglenews==0.1.0, pygooglenews==0.1.1 and pygooglenews==0.1.2 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\n"
     ]
    }
   ],
   "source": [
    "pip install pygooglenews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "af604528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaperNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading newspaper-0.1.0.7.tar.gz (176 kB)\n",
      "  Downloading newspaper-0.1.0.6.tar.gz (176 kB)\n",
      "Collecting beautifulsoup4==4.3.2\n",
      "  Downloading beautifulsoup4-4.3.2.tar.gz (143 kB)\n",
      "Collecting Pillow==2.5.1\n",
      "  Downloading Pillow-2.5.1.zip (6.9 MB)\n",
      "Collecting PyYAML==3.11\n",
      "  Downloading PyYAML-3.11.zip (371 kB)\n",
      "Collecting cssselect==0.9.1\n",
      "  Downloading cssselect-0.9.1.tar.gz (32 kB)\n",
      "Collecting lxml==3.3.5\n",
      "  Downloading lxml-3.3.5.tar.gz (3.5 MB)\n",
      "Collecting nltk==2.0.5\n",
      "  Downloading nltk-2.0.5.zip (1.1 MB)\n",
      "  Downloading nltk-2.0.5.tar.gz (954 kB)\n",
      "Collecting newspaper\n",
      "  Downloading newspaper-0.1.0.5.tar.gz (49 kB)\n",
      "  Downloading newspaper-0.1.0.4.tar.gz (49 kB)\n",
      "  Downloading newspaper-0.1.0.3.tar.gz (49 kB)\n",
      "  Downloading newspaper-0.1.0.2.tar.gz (180 kB)\n",
      "  Downloading newspaper-0.1.0.1.tar.gz (49 kB)\n",
      "  Downloading newspaper-0.1.0.0.tar.gz (49 kB)\n",
      "  Downloading newspaper-0.0.9.9.tar.gz (49 kB)\n",
      "  Downloading newspaper-0.0.9.8.tar.gz (248 kB)\n",
      "  Downloading newspaper-0.0.9.6.tar.gz (244 kB)\n",
      "Collecting nltk==2.0.4\n",
      "  Downloading nltk-2.0.4.zip (1.1 MB)\n",
      "  Downloading nltk-2.0.4.tar.gz (955 kB)\n",
      "Collecting newspaper\n",
      "\n",
      "  Downloading newspaper-0.0.9.5.tar.gz (244 kB)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_07706c22604743379cde69ef2d87cb49\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_07706c22604743379cde69ef2d87cb49\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-lm2nh3_b'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_07706c22604743379cde69ef2d87cb49\\\n",
      "    Complete output (1 lines):\n",
      "    WARNING! You are attempting to install newspaper's python2 repository on python3. PLEASE RUN `$ pip3 install newspaper3k` for python3 or `$ pip install newspaper` for python2\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/d8/07/5765cc9c36e2be1a0f0e615b7a092129e1ba30a25182506dea437290c193/newspaper-0.1.0.7.tar.gz#sha256=929ea447660d2633d3eea6c2aba703b549f7cdd56bd5cf636eb8f1454b254945 (from https://pypi.org/simple/newspaper/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\nltk_a0017d91597c42afa11ddce94ea2f591\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\nltk_a0017d91597c42afa11ddce94ea2f591\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-toy2m6u9'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_a0017d91597c42afa11ddce94ea2f591\\\n",
      "    Complete output (31 lines):\n",
      "    Downloading http://pypi.python.org/packages/source/d/distribute/distribute-0.6.21.tar.gz\n",
      "    Traceback (most recent call last):\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_a0017d91597c42afa11ddce94ea2f591\\distribute_setup.py\", line 143, in use_setuptools\n",
      "        raise ImportError\n",
      "    ImportError\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_a0017d91597c42afa11ddce94ea2f591\\setup.py\", line 23, in <module>\n",
      "        distribute_setup.use_setuptools()\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_a0017d91597c42afa11ddce94ea2f591\\distribute_setup.py\", line 145, in use_setuptools\n",
      "        return _do_download(version, download_base, to_dir, download_delay)\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_a0017d91597c42afa11ddce94ea2f591\\distribute_setup.py\", line 123, in _do_download\n",
      "        tarball = download_setuptools(version, download_base,\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_a0017d91597c42afa11ddce94ea2f591\\distribute_setup.py\", line 193, in download_setuptools\n",
      "        src = urlopen(url)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 214, in urlopen\n",
      "        return opener.open(url, data, timeout)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 523, in open\n",
      "        response = meth(req, response)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 632, in http_response\n",
      "        response = self.parent.error(\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 561, in error\n",
      "        return self._call_chain(*args)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 494, in _call_chain\n",
      "        result = func(*args)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 641, in http_error_default\n",
      "        raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "    urllib.error.HTTPError: HTTP Error 403: SSL is required\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/61/32/c7d454c7d232034dfb492b6ef5d749b367c58e0a0f2ed4125eb4089b9227/nltk-2.0.5.zip#sha256=edbeb82d03ef6fe24d4ec8380998d23cb2e36eb43350c9f33e932fe07de42cd5 (from https://pypi.org/simple/nltk/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\nltk_2e7707ab279844ec8f3f5f890cf9bff1\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\nltk_2e7707ab279844ec8f3f5f890cf9bff1\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-m8h_5ozc'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_2e7707ab279844ec8f3f5f890cf9bff1\\\n",
      "    Complete output (31 lines):\n",
      "    Downloading http://pypi.python.org/packages/source/d/distribute/distribute-0.6.21.tar.gz\n",
      "    Traceback (most recent call last):\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_2e7707ab279844ec8f3f5f890cf9bff1\\distribute_setup.py\", line 143, in use_setuptools\n",
      "        raise ImportError\n",
      "    ImportError\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_2e7707ab279844ec8f3f5f890cf9bff1\\setup.py\", line 23, in <module>\n",
      "        distribute_setup.use_setuptools()\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_2e7707ab279844ec8f3f5f890cf9bff1\\distribute_setup.py\", line 145, in use_setuptools\n",
      "        return _do_download(version, download_base, to_dir, download_delay)\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_2e7707ab279844ec8f3f5f890cf9bff1\\distribute_setup.py\", line 123, in _do_download\n",
      "        tarball = download_setuptools(version, download_base,\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_2e7707ab279844ec8f3f5f890cf9bff1\\distribute_setup.py\", line 193, in download_setuptools\n",
      "        src = urlopen(url)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 214, in urlopen\n",
      "        return opener.open(url, data, timeout)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 523, in open\n",
      "        response = meth(req, response)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 632, in http_response\n",
      "        response = self.parent.error(\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 561, in error\n",
      "        return self._call_chain(*args)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 494, in _call_chain\n",
      "        result = func(*args)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 641, in http_error_default\n",
      "        raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "    urllib.error.HTTPError: HTTP Error 403: SSL is required\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/50/9e/39418026bf8013bbc2852c7aec3fb21e4339f6cd694934496d67a19b53b8/nltk-2.0.5.tar.gz#sha256=590b1752fb39427ad8018a65f72355c22e8276d476699aa4a488e38c1b17889a (from https://pypi.org/simple/nltk/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_c85cbe1ceb0a40f2ba90b1ec7895861c\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_c85cbe1ceb0a40f2ba90b1ec7895861c\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-eissosuo'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_c85cbe1ceb0a40f2ba90b1ec7895861c\\\n",
      "    Complete output (1 lines):\n",
      "    WARNING! You are attempting to install newspaper's python2 repository on python3. PLEASE RUN `$ pip3 install newspaper3k` for python3 or `$ pip install newspaper` for python2\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/57/21/35a7e7040e70a628b526c6830e6f506868f54dcaadffbe57cb3d393af5cb/newspaper-0.1.0.5.tar.gz#sha256=b346e615f5a0c8ac5ab1ebab2420ad24fda7fc78198cae32f1b3fed640dde788 (from https://pypi.org/simple/newspaper/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_65768048514e41cc8e73b4c2a30e934f\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_65768048514e41cc8e73b4c2a30e934f\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-jr2_92oy'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_65768048514e41cc8e73b4c2a30e934f\\\n",
      "    Complete output (1 lines):\n",
      "    WARNING! You are attempting to install newspaper's python2 repository on python3. PLEASE RUN `$ pip3 install newspaper3k` for python3 or `$ pip install newspaper` for python2\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/da/a3/88885ac6e2f65b84f23511da21bdd486d63d2a035d0adab6e4ef89cd4c25/newspaper-0.1.0.4.tar.gz#sha256=927edcccb7d57905fbd24fde4fb3879ffcc59d082877ac7c40963f795372cb67 (from https://pypi.org/simple/newspaper/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_3a41cb4cba354748a78827532dc9fcdc\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_3a41cb4cba354748a78827532dc9fcdc\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-xi0zk03f'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_3a41cb4cba354748a78827532dc9fcdc\\\n",
      "    Complete output (1 lines):\n",
      "    WARNING! You are attempting to install newspaper's python2 repository on python3. PLEASE RUN `$ pip3 install newspaper3k` for python3 or `$ pip install newspaper` for python2\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/4f/59/140bdf8b9413c552fdfa73eb01b6622eaa8ed62ded2f390c4062b58f58bb/newspaper-0.1.0.3.tar.gz#sha256=d2eb46563c973a4046e82580d6193df42b86bff12cbfcb39e9780b0f7dc24595 (from https://pypi.org/simple/newspaper/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_5b2e1913baad4e8884148fee0e616d5e\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_5b2e1913baad4e8884148fee0e616d5e\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-bju5u5bp'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_5b2e1913baad4e8884148fee0e616d5e\\\n",
      "    Complete output (1 lines):\n",
      "    WARNING! You are attempting to install newspaper's python2 repository on python3. PLEASE RUN `$ pip3 install newspaper3k` for python3 or `$ pip install newspaper` for python2\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/9c/57/e6af9770c3241725fbf7a79c6f2954988947202e2cab01cdf53457f039a8/newspaper-0.1.0.1.tar.gz#sha256=bc40cf56e35f0c6fe79b77f3a4c84a950de9f66559a51bd280e3c363796afc75 (from https://pypi.org/simple/newspaper/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_0720b3aa5e3641dd85a68f6387b5b634\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_0720b3aa5e3641dd85a68f6387b5b634\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-dpomo_mu'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_0720b3aa5e3641dd85a68f6387b5b634\\\n",
      "    Complete output (1 lines):\n",
      "    WARNING! You are attempting to install newspaper's python2 repository on python3. PLEASE RUN `$ pip3 install newspaper3k` for python3 or `$ pip install newspaper` for python2\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/be/7b/a34b526cf37727ef353ff46217173a67a598fbbf3d8dbd80b899e51af45c/newspaper-0.1.0.0.tar.gz#sha256=8c71680c09c39642be06d632149fe0e44a93ae5494e9f98c19fc79d6d21817bf (from https://pypi.org/simple/newspaper/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_3995cfbd0e974541a04c8c2d56f0fcc4\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_3995cfbd0e974541a04c8c2d56f0fcc4\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-x3ynnqgo'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_3995cfbd0e974541a04c8c2d56f0fcc4\\\n",
      "    Complete output (1 lines):\n",
      "    WARNING! You are attempting to install newspaper's python2 repository on python3. PLEASE RUN `$ pip3 install newspaper3k` for python3 or `$ pip install newspaper` for python2\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/ad/b6/02023ed7543c280cbd18eef61502b97f63eed842a1414f7596e370a97362/newspaper-0.0.9.9.tar.gz#sha256=e2449afd697d9280c78301e4d1e385fd7d6dc73e0a4a130c35f0e909f1804402 (from https://pypi.org/simple/newspaper/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\nltk_58d86309e81c484ab94b84157fa5fac9\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\nltk_58d86309e81c484ab94b84157fa5fac9\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-lo9__i16'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_58d86309e81c484ab94b84157fa5fac9\\\n",
      "    Complete output (31 lines):\n",
      "    Downloading http://pypi.python.org/packages/source/d/distribute/distribute-0.6.21.tar.gz\n",
      "    Traceback (most recent call last):\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_58d86309e81c484ab94b84157fa5fac9\\distribute_setup.py\", line 143, in use_setuptools\n",
      "        raise ImportError\n",
      "    ImportError\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_58d86309e81c484ab94b84157fa5fac9\\setup.py\", line 23, in <module>\n",
      "        distribute_setup.use_setuptools()\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_58d86309e81c484ab94b84157fa5fac9\\distribute_setup.py\", line 145, in use_setuptools\n",
      "        return _do_download(version, download_base, to_dir, download_delay)\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_58d86309e81c484ab94b84157fa5fac9\\distribute_setup.py\", line 123, in _do_download\n",
      "        tarball = download_setuptools(version, download_base,\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_58d86309e81c484ab94b84157fa5fac9\\distribute_setup.py\", line 193, in download_setuptools\n",
      "        src = urlopen(url)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 214, in urlopen\n",
      "        return opener.open(url, data, timeout)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 523, in open\n",
      "        response = meth(req, response)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 632, in http_response\n",
      "        response = self.parent.error(\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 561, in error\n",
      "        return self._call_chain(*args)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 494, in _call_chain\n",
      "        result = func(*args)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 641, in http_error_default\n",
      "        raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "    urllib.error.HTTPError: HTTP Error 403: SSL is required\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/25/a7/f8938bdf6f6d17e37e882b67f1362c0fdb6a8a609a69252b6033d6c546c5/nltk-2.0.4.zip#sha256=70936b0cf30710a1d63df152c3286af39af37b131cd176c0202e6665a8630632 (from https://pypi.org/simple/nltk/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\nltk_a3842c6e82a7482eb84909dc678b1ca9\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\nltk_a3842c6e82a7482eb84909dc678b1ca9\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-dfz5nyzv'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_a3842c6e82a7482eb84909dc678b1ca9\\\n",
      "    Complete output (31 lines):\n",
      "    Downloading http://pypi.python.org/packages/source/d/distribute/distribute-0.6.21.tar.gz\n",
      "    Traceback (most recent call last):\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_a3842c6e82a7482eb84909dc678b1ca9\\distribute_setup.py\", line 143, in use_setuptools\n",
      "        raise ImportError\n",
      "    ImportError\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_a3842c6e82a7482eb84909dc678b1ca9\\setup.py\", line 23, in <module>\n",
      "        distribute_setup.use_setuptools()\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_a3842c6e82a7482eb84909dc678b1ca9\\distribute_setup.py\", line 145, in use_setuptools\n",
      "        return _do_download(version, download_base, to_dir, download_delay)\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_a3842c6e82a7482eb84909dc678b1ca9\\distribute_setup.py\", line 123, in _do_download\n",
      "        tarball = download_setuptools(version, download_base,\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\nltk_a3842c6e82a7482eb84909dc678b1ca9\\distribute_setup.py\", line 193, in download_setuptools\n",
      "        src = urlopen(url)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 214, in urlopen\n",
      "        return opener.open(url, data, timeout)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 523, in open\n",
      "        response = meth(req, response)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 632, in http_response\n",
      "        response = self.parent.error(\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 561, in error\n",
      "        return self._call_chain(*args)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 494, in _call_chain\n",
      "        result = func(*args)\n",
      "      File \"C:\\Users\\michaelpellegrino\\Anaconda3\\lib\\urllib\\request.py\", line 641, in http_error_default\n",
      "        raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "    urllib.error.HTTPError: HTTP Error 403: SSL is required\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/81/f6/30c4eb35ad7a4b5e9301943c8738b79ebb8152f0986e877f809b8e295c61/nltk-2.0.4.tar.gz#sha256=a554d6b9c5c7c8b597a090d8848a6f78c6fc4665ae43c9a6a6d6a5b207d98c65 (from https://pypi.org/simple/nltk/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\beautifulsoup_ba170a790e2241148e1e1ae0a301dccd\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\beautifulsoup_ba170a790e2241148e1e1ae0a301dccd\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-44_tciki'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\beautifulsoup_ba170a790e2241148e1e1ae0a301dccd\\\n",
      "    Complete output (6 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\beautifulsoup_ba170a790e2241148e1e1ae0a301dccd\\setup.py\", line 22\n",
      "        print \"Unit tests have failed!\"\n",
      "              ^\n",
      "    SyntaxError: Missing parentheses in call to 'print'. Did you mean print(\"Unit tests have failed!\")?\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/1e/ee/295988deca1a5a7accd783d0dfe14524867e31abb05b6c0eeceee49c759d/BeautifulSoup-3.2.1.tar.gz#sha256=6a8cb4401111e011b579c8c52a51cdab970041cc543814bbd9577a4529fe1cdb (from https://pypi.org/simple/beautifulsoup/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\beautifulsoup_3fda0478571743f9b607205fb0811830\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\beautifulsoup_3fda0478571743f9b607205fb0811830\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-3pyhw_vv'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\beautifulsoup_3fda0478571743f9b607205fb0811830\\\n",
      "    Complete output (6 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\beautifulsoup_3fda0478571743f9b607205fb0811830\\setup.py\", line 3\n",
      "        \"You're trying to run a very old release of Beautiful Soup under Python 3. This will not work.\"<>\"Please use Beautiful Soup 4, available through the pip package 'beautifulsoup4'.\"\n",
      "                                                                                                       ^\n",
      "    SyntaxError: invalid syntax\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/40/f2/6c9f2f3e696ee6a1fb0e4d7850617e224ed2b0b1e872110abffeca2a09d4/BeautifulSoup-3.2.2.tar.gz#sha256=a04169602bff6e3138b1259dbbf491f5a27f9499dea9a8fbafd48843f9d89970 (from https://pypi.org/simple/beautifulsoup/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\beautifulsoup_eb2ccc6f214d4ab6bacee2193a725382\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\beautifulsoup_eb2ccc6f214d4ab6bacee2193a725382\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-7mhbjlev'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\beautifulsoup_eb2ccc6f214d4ab6bacee2193a725382\\\n",
      "    Complete output (6 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\beautifulsoup_eb2ccc6f214d4ab6bacee2193a725382\\setup.py\", line 22\n",
      "        print \"Unit tests have failed!\"\n",
      "              ^\n",
      "    SyntaxError: Missing parentheses in call to 'print'. Did you mean print(\"Unit tests have failed!\")?\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/33/fe/15326560884f20d792d3ffc7fe8f639aab88647c9d46509a240d9bfbb6b1/BeautifulSoup-3.2.0.tar.gz#sha256=0dc52d07516c1665c9dd9f0a390a7a054bfb7b147a50b2866fb116b8909dfd37 (from https://pypi.org/simple/beautifulsoup/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_03fc580fde0243b5bf803b1df9afac24\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_03fc580fde0243b5bf803b1df9afac24\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-xti_z1qk'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_03fc580fde0243b5bf803b1df9afac24\\\n",
      "    Complete output (6 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_03fc580fde0243b5bf803b1df9afac24\\setup.py\", line 57\n",
      "        print ''\n",
      "              ^\n",
      "    SyntaxError: Missing parentheses in call to 'print'. Did you mean print('')?\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/6f/ed/2a56827d9291bd27251d60bfd11affd3dce7d24ba929014bbd4975a17eca/newspaper-0.0.7.tar.gz#sha256=a29c39e9c1c142238249208aed07204047c4a0b1cdcc92138dc98953cf66f7c3 (from https://pypi.org/simple/newspaper/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_de9cb535101646fc802887a78b3d93b0\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_de9cb535101646fc802887a78b3d93b0\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-sily82kv'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_de9cb535101646fc802887a78b3d93b0\\\n",
      "    Complete output (6 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_de9cb535101646fc802887a78b3d93b0\\setup.py\", line 60\n",
      "        print ''\n",
      "              ^\n",
      "    SyntaxError: Missing parentheses in call to 'print'. Did you mean print('')?\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/d7/7f/ddcd5263d1fa14936786b01f62e75a4f806f98cb2255587af00ccb08b97b/newspaper-0.0.6.tar.gz#sha256=7bb92ecf4ef26dc3121b798e933dfc4cce3eb95d1d9e4ae01887231f12e30f9d (from https://pypi.org/simple/newspaper/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_458aceebbf824e7aa286bc4cb7a69845\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_458aceebbf824e7aa286bc4cb7a69845\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-l4cn7d9s'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_458aceebbf824e7aa286bc4cb7a69845\\\n",
      "    Complete output (6 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_458aceebbf824e7aa286bc4cb7a69845\\setup.py\", line 60\n",
      "        print ''\n",
      "              ^\n",
      "    SyntaxError: Missing parentheses in call to 'print'. Did you mean print('')?\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/de/4b/3279534d13073ea8958edff3b97e39e853faec4436e1c9d805f9cf6e9f23/newspaper-0.0.5.tar.gz#sha256=7fe4fa9121bb07707fa2489479ab7994a056ca9cc3a848ed634d5537c7eaa0bc (from https://pypi.org/simple/newspaper/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_f2a16c40471e4f90bdc916dfa123c72a\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_f2a16c40471e4f90bdc916dfa123c72a\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-lrefg_qr'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_f2a16c40471e4f90bdc916dfa123c72a\\\n",
      "    Complete output (6 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_f2a16c40471e4f90bdc916dfa123c72a\\setup.py\", line 61\n",
      "        print ''\n",
      "              ^\n",
      "    SyntaxError: Missing parentheses in call to 'print'. Did you mean print('')?\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/44/10/cc8abed3de450ea2925601e29951eec9658a19f18572429cc29380ec7ac8/newspaper-0.0.4.tar.gz#sha256=359934ee0c47015687ac3b71d51c7d1a87e8b95ff96135bdbe5c4d2e2c20c735 (from https://pypi.org/simple/newspaper/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\michaelpellegrino\\Anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_1baaea0c00db45c1a181230b1ff75e20\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\michaelpellegrino\\\\AppData\\\\Local\\\\Temp\\\\pip-install-zcricdpw\\\\newspaper_1baaea0c00db45c1a181230b1ff75e20\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-pip-egg-info-3id180f7'\n",
      "         cwd: C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_1baaea0c00db45c1a181230b1ff75e20\\\n",
      "    Complete output (6 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\michaelpellegrino\\AppData\\Local\\Temp\\pip-install-zcricdpw\\newspaper_1baaea0c00db45c1a181230b1ff75e20\\setup.py\", line 88\n",
      "        print 'Please run:',\"curl https://raw.github.com/codelucas/newspaper/master/download_corpora.py | python\", 'to download the required nltk corpora'\n",
      "              ^\n",
      "    SyntaxError: Missing parentheses in call to 'print'. Did you mean print('Please run:',\"curl https://raw.github.com/codelucas/newspaper/master/download_corpora.py | python\", 'to download the required nltk corpora')?\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/6f/d3/c73ee757a0d2cd493a7d1ad16482902761ca5be3a52d971bce0cbf17b6fd/newspaper-0.0.2.tar.gz#sha256=a94c0427bc20448242457873d9dddb8c872ce82a6fb342b90b72bec790a117ab (from https://pypi.org/simple/newspaper/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n",
      "ERROR: Cannot install newspaper==0.0.3, newspaper==0.0.8, newspaper==0.0.9, newspaper==0.0.9.1, newspaper==0.0.9.2, newspaper==0.0.9.5, newspaper==0.0.9.6, newspaper==0.0.9.8, newspaper==0.1.0.2 and newspaper==0.1.0.6 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading newspaper-0.0.9.2.tar.gz (242 kB)\n",
      "  Downloading newspaper-0.0.9.1.tar.gz (760 kB)\n",
      "Collecting BeautifulSoup==3.2.1\n",
      "  Downloading BeautifulSoup-3.2.1.tar.gz (31 kB)\n",
      "Collecting newspaper\n",
      "  Downloading newspaper-0.0.9.tar.gz (760 kB)\n",
      "  Downloading newspaper-0.0.8.tar.gz (6.9 MB)\n",
      "Requirement already satisfied: lxml in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from newspaper) (4.9.2)\n",
      "Requirement already satisfied: requests in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from newspaper) (2.27.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from newspaper) (3.7)\n",
      "Requirement already satisfied: Pillow in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from newspaper) (9.0.1)\n",
      "Requirement already satisfied: cssselect in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from newspaper) (1.2.0)\n",
      "Collecting BeautifulSoup\n",
      "  Downloading BeautifulSoup-3.2.2.tar.gz (32 kB)\n",
      "  Downloading BeautifulSoup-3.2.0.tar.gz (31 kB)\n",
      "Collecting newspaper\n",
      "  Downloading newspaper-0.0.7.tar.gz (6.9 MB)\n",
      "  Downloading newspaper-0.0.6.tar.gz (7.0 MB)\n",
      "  Downloading newspaper-0.0.5.tar.gz (7.7 MB)\n",
      "  Downloading newspaper-0.0.4.tar.gz (6.7 MB)\n",
      "  Downloading newspaper-0.0.3.tar.gz (10.8 MB)\n",
      "Collecting lxml==3.2.4\n",
      "  Downloading lxml-3.2.4.tar.gz (3.3 MB)\n",
      "Collecting newspaper\n",
      "  Downloading newspaper-0.0.2.tar.gz (10.8 MB)\n",
      "\n",
      "The conflict is caused by:\n",
      "    newspaper 0.1.0.6 depends on nltk==2.0.5\n",
      "    newspaper 0.1.0.2 depends on nltk==2.0.5\n",
      "    newspaper 0.0.9.8 depends on nltk==2.0.5\n",
      "    newspaper 0.0.9.6 depends on nltk==2.0.4\n",
      "    newspaper 0.0.9.5 depends on nltk==2.0.4\n",
      "    newspaper 0.0.9.2 depends on nltk==2.0.4\n",
      "    newspaper 0.0.9.1 depends on BeautifulSoup==3.2.1\n",
      "    newspaper 0.0.9 depends on BeautifulSoup==3.2.1\n",
      "    newspaper 0.0.8 depends on BeautifulSoup\n",
      "    newspaper 0.0.3 depends on BeautifulSoup\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install newspaper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "91d220d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'newspaper'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnewspaper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Article\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msentiment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentimentIntensityAnalyzer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Download the VADER lexicon for sentiment analysis\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'newspaper'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from newspaper import Article\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download the VADER lexicon for sentiment analysis\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def analyze_article_sentiment(url):\n",
    "    # Initialize the sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Download and parse the article\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "\n",
    "    # Get the article text content\n",
    "    article_text = article.text\n",
    "\n",
    "    # Analyze the sentiment of the article text\n",
    "    sentiment_scores = sid.polarity_scores(article_text)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "\n",
    "    # Determine the sentiment label based on the compound score\n",
    "    sentiment_label = 'positive' if compound_score >= 0 else 'negative'\n",
    "\n",
    "    return sentiment_label\n",
    "\n",
    "# Example usage\n",
    "article_url = 'https://www.bloomberg.com/news/articles/2023-06-12/apple-closes-at-record-in-latest-sign-of-big-tech-s-dominance#xj4y7vzkg'\n",
    "sentiment = analyze_article_sentiment(article_url)\n",
    "print(f\"Sentiment of the article: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfc6b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
