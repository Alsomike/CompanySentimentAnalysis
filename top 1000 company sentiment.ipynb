{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b917eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82bc1a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from newspaper3k) (4.9.2)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from newspaper3k) (3.7)\n",
      "Collecting jieba3k>=0.35.1\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from newspaper3k) (9.0.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from newspaper3k) (4.11.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from newspaper3k) (2.8.2)\n",
      "Collecting tinysegmenter==0.3\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from newspaper3k) (3.2.0)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from newspaper3k) (2.27.1)\n",
      "Collecting feedfinder2>=0.0.4\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "Collecting feedparser>=5.2.1\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from newspaper3k) (6.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "Requirement already satisfied: click in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.26.9)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.6.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\michaelpellegrino\\anaconda3\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.4)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for tinysegmenter (setup.py): started\n",
      "  Building wheel for tinysegmenter (setup.py): finished with status 'done'\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13553 sha256=ec11193070701d73ccfd01d40eb3d3fdbaecfa89ac3ccca7a05a3c939fe556a9\n",
      "  Stored in directory: c:\\users\\michaelpellegrino\\appdata\\local\\pip\\cache\\wheels\\94\\ad\\df\\a2a01300cea47d5695f242f7e925a805970106fd9e4b151468\n",
      "  Building wheel for feedfinder2 (setup.py): started\n",
      "  Building wheel for feedfinder2 (setup.py): finished with status 'done'\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3357 sha256=70ca745b1fa6daee677ea02af2e8d7cc687183e93202119bc7d600be359286d7\n",
      "  Stored in directory: c:\\users\\michaelpellegrino\\appdata\\local\\pip\\cache\\wheels\\43\\4a\\c2\\61a371b2524ac90805391c660d8dc4505705297f25e2b85a5d\n",
      "  Building wheel for jieba3k (setup.py): started\n",
      "  Building wheel for jieba3k (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398404 sha256=125921036dfd94b4787e9ad65b32e86482bff87d2169bed88093b476c39d15bd\n",
      "  Stored in directory: c:\\users\\michaelpellegrino\\appdata\\local\\pip\\cache\\wheels\\c2\\22\\59\\8214a8d6357e9f540ce1f37f9a4362b6156b4ca81b37f1945f\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=5ba8705b10004f3b782ff96c46b28820c3e9b84d92abaaefd39da54ac4eb7f18\n",
      "  Stored in directory: c:\\users\\michaelpellegrino\\appdata\\local\\pip\\cache\\wheels\\65\\7a\\a7\\78c287f64e401255dff4c13fdbc672fed5efbfd21c530114e1\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: sgmllib3k, tinysegmenter, jieba3k, feedparser, feedfinder2, newspaper3k\n",
      "Successfully installed feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3k-0.2.8 sgmllib3k-1.0.0 tinysegmenter-0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5662f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\michaelpellegrino\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment of the article: positive\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from newspaper import Article\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download the VADER lexicon for sentiment analysis\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def analyze_article_sentiment(url):\n",
    "    # Initialize the sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Download and parse the article\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "\n",
    "    # Get the article text content\n",
    "    article_text = article.text\n",
    "\n",
    "    # Analyze the sentiment of the article text\n",
    "    sentiment_scores = sid.polarity_scores(article_text)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "\n",
    "    # Determine the sentiment label based on the compound score\n",
    "    sentiment_label = 'positive' if compound_score >= 0 else 'negative'\n",
    "\n",
    "    return sentiment_label\n",
    "\n",
    "# Example usage\n",
    "article_url = 'https://www.bloomberg.com/news/articles/2023-06-12/apple-closes-at-record-in-latest-sign-of-big-tech-s-dominance#xj4y7vzkg'\n",
    "sentiment = analyze_article_sentiment(article_url)\n",
    "print(f\"Sentiment of the article: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bb5b02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search query: microsoft\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_google_news(query):\n",
    "    search_query = query + \"+Company\"\n",
    "    search_url = f\"https://www.google.com/search?q={search_query}&tbm=nws\"\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    search_results = soup.find_all('div', class_='dbsr')\n",
    "    result_urls = []\n",
    "\n",
    "    for result in search_results:\n",
    "        link = result.find('a')\n",
    "        if link:\n",
    "            result_urls.append(link['href'])\n",
    "\n",
    "    return result_urls\n",
    "\n",
    "# Example usage\n",
    "search_input = input(\"Enter a search query: \")\n",
    "news_urls = search_google_news(search_input)\n",
    "\n",
    "# Print the URLs of the search results\n",
    "for url in news_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cd54db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search query: apple\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_google_news(query):\n",
    "    search_query = query + \"+Company\"\n",
    "    search_url = f\"https://www.google.com/search?q={search_query}&tbm=nws\"\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    search_results = soup.find_all('div', class_='dbsr')\n",
    "    result_urls = []\n",
    "\n",
    "    for result in search_results:\n",
    "        link = result.find('a')\n",
    "        if link:\n",
    "            result_urls.append(link['href'])\n",
    "\n",
    "    return result_urls\n",
    "\n",
    "# Example usage\n",
    "search_input = input(\"Enter a search query: \")\n",
    "news_urls = search_google_news(search_input)\n",
    "\n",
    "# Print the URLs of the search results\n",
    "for url in news_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "264fcb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search query: apple\n",
      "https://www.bloomberg.com/news/articles/2023-06-12/apple-closes-at-record-in-latest-sign-of-big-tech-s-dominance\n",
      "https://www.thestreet.com/cryptocurrency/apple-issues-warning-to-company-to-remove-bitcoin-payments-feature\n",
      "https://www.marketwatch.com/story/apple-inc-stock-rises-wednesday-still-underperforms-market-1b65a190-3ba63c6a2812%3Fmod%3Dnewsviewer_click\n",
      "https://www.theverge.com/2023/6/6/23751350/apple-mira-ar-headset-startup\n",
      "https://www.barrons.com/articles/apple-iphone-15-upgrades-stock-a187c9e6\n",
      "https://www.fastcompany.com/90905495/apple-no-longer-design-led-company\n",
      "https://www.reuters.com/legal/apple-amazon-must-face-consumer-lawsuit-over-iphone-ipad-prices-us-judge-2023-06-09/\n",
      "https://finance.yahoo.com/news/apples-lesser-known-co-founder-180031182.html\n",
      "https://www.usatoday.com/story/tech/2023/06/05/wwdc-2023-apple-keynote-live-updates/70287849007/\n",
      "https://www.theregister.com/2023/05/19/apple_chatgpt/\n",
      "https://support.google.com/websearch%3Fp%3Dws_settings_location%26hl%3Den\n",
      "https://accounts.google.com/ServiceLogin%3Fcontinue%3Dhttps://www.google.com/search%253Fq%253Dapple%252BCompany%2526tbm%253Dnws%26hl%3Den\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def search_google_news(query):\n",
    "    search_query = query + \" Company\"\n",
    "    search_url = f\"https://www.google.com/search?q={search_query}&tbm=nws\"\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    search_results = soup.find_all('a', href=True)\n",
    "    result_urls = []\n",
    "\n",
    "    for result in search_results:\n",
    "        url = result['href']\n",
    "        if url.startswith('/url?q='):\n",
    "            url = url[7:]  # Remove '/url?q=' prefix\n",
    "            url = re.split('&', url, maxsplit=1)[0]  # Remove additional parameters\n",
    "            result_urls.append(url)\n",
    "\n",
    "    return result_urls\n",
    "\n",
    "# Example usage\n",
    "search_input = input(\"Enter a search query: \")\n",
    "news_urls = search_google_news(search_input)\n",
    "\n",
    "# Print the URLs of the search results\n",
    "for url in news_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ea36080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Articles Count: 3\n",
      "Negative Articles Count: 0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from newspaper import Article\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyze_sentiment(urls):\n",
    "    # Initialize the sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "\n",
    "    for url in urls:\n",
    "        # Download and parse the article\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        # Get the article text content\n",
    "        article_text = article.text\n",
    "\n",
    "        # Analyze the sentiment of the article text\n",
    "        sentiment_scores = sid.polarity_scores(article_text)\n",
    "        compound_score = sentiment_scores['compound']\n",
    "\n",
    "        # Determine the sentiment label based on the compound score\n",
    "        sentiment_label = 'positive' if compound_score >= 0 else 'negative'\n",
    "\n",
    "        # Increment the corresponding sentiment count\n",
    "        if sentiment_label == 'positive':\n",
    "            positive_count += 1\n",
    "        else:\n",
    "            negative_count += 1\n",
    "\n",
    "    return positive_count, negative_count\n",
    "\n",
    "# Example usage\n",
    "article_urls = [\n",
    "    'https://www.bloomberg.com/news/articles/2023-06-12/apple-closes-at-record-in-latest-sign-of-big-tech-s-dominance#xj4y7vzkg',\n",
    "    'https://www.marketwatch.com/data-news/apple-inc-stock-rises-wednesday-still-underperforms-market-1b65a190-3ba63c6a2812-5f0339f1785f?mod=newsviewer_click&tesla=y',\n",
    "    'https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=newssearch&cd=&cad=rja&uact=8&ved=2ahUKEwjj2KfnrMX_AhXAg4kEHS1vAgwQxfQBKAB6BAgMEAE&url=https%3A%2F%2Fwww.theverge.com%2F2023%2F6%2F6%2F23751350%2Fapple-mira-ar-headset-startup&usg=AOvVaw2IrxWPeyTXLKOT0hkLtbvr'\n",
    "]\n",
    "\n",
    "positive_count, negative_count = analyze_sentiment(article_urls)\n",
    "\n",
    "print(f\"Positive Articles Count: {positive_count}\")\n",
    "print(f\"Negative Articles Count: {negative_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "323f4b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while processing the URL: https://www.thestreet.com/cryptocurrency/apple-issues-warning-to-company-to-remove-bitcoin-payments-feature\n",
      "Error message: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.thestreet.com/cryptocurrency/apple-issues-warning-to-company-to-remove-bitcoin-payments-feature on URL https://www.thestreet.com/cryptocurrency/apple-issues-warning-to-company-to-remove-bitcoin-payments-feature\n",
      "Positive Articles Count: 3\n",
      "Negative Articles Count: 0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from newspaper import Article\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyze_sentiment(urls):\n",
    "    # Initialize the sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Download and parse the article\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            # Get the article text content\n",
    "            article_text = article.text\n",
    "\n",
    "            # Analyze the sentiment of the article text\n",
    "            sentiment_scores = sid.polarity_scores(article_text)\n",
    "            compound_score = sentiment_scores['compound']\n",
    "\n",
    "            # Determine the sentiment label based on the compound score\n",
    "            sentiment_label = 'positive' if compound_score >= 0 else 'negative'\n",
    "\n",
    "            # Increment the corresponding sentiment count\n",
    "            if sentiment_label == 'positive':\n",
    "                positive_count += 1\n",
    "            else:\n",
    "                negative_count += 1\n",
    "        except Exception as e:\n",
    "            # Handle the exception, you can print the error message or handle it as per your requirement\n",
    "            print(f\"Error occurred while processing the URL: {url}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return positive_count, negative_count\n",
    "\n",
    "# Example usage\n",
    "article_urls = [\n",
    "    'https://www.bloomberg.com/news/articles/2023-06-12/apple-closes-at-record-in-latest-sign-of-big-tech-s-dominance#xj4y7vzkg',\n",
    "    'https://www.marketwatch.com/data-news/apple-inc-stock-rises-wednesday-still-underperforms-market-1b65a190-3ba63c6a2812-5f0339f1785f?mod=newsviewer_click&tesla=y',\n",
    "    'https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=newssearch&cd=&cad=rja&uact=8&ved=2ahUKEwjj2KfnrMX_AhXAg4kEHS1vAgwQxfQBKAB6BAgMEAE&url=https%3A%2F%2Fwww.theverge.com%2F2023%2F6%2F6%2F23751350%2Fapple-mira-ar-headset-startup&usg=AOvVaw2IrxWPeyTXLKOT0hkLtbvr',\n",
    "    'https://www.thestreet.com/cryptocurrency/apple-issues-warning-to-company-to-remove-bitcoin-payments-feature'\n",
    "]\n",
    "\n",
    "positive_count, negative_count = analyze_sentiment(article_urls)\n",
    "\n",
    "print(f\"Positive Articles Count: {positive_count}\")\n",
    "print(f\"Negative Articles Count: {negative_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c515270",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 64>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_urls\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m search_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter a search query: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m news_urls \u001b[38;5;241m=\u001b[39m search_google_news(search_input)\n\u001b[0;32m     67\u001b[0m positive_count, negative_count \u001b[38;5;241m=\u001b[39m analyze_sentiment(news_urls)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m     )\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyze_sentiment(urls):\n",
    "    # Initialize the sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Download and parse the article\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            # Get the article text content\n",
    "            article_text = article.text\n",
    "\n",
    "            # Analyze the sentiment of the article text\n",
    "            sentiment_scores = sid.polarity_scores(article_text)\n",
    "            compound_score = sentiment_scores['compound']\n",
    "\n",
    "            # Determine the sentiment label based on the compound score\n",
    "            sentiment_label = 'positive' if compound_score >= 0 else 'negative'\n",
    "\n",
    "            # Increment the corresponding sentiment count\n",
    "            if sentiment_label == 'positive':\n",
    "                positive_count += 1\n",
    "            else:\n",
    "                negative_count += 1\n",
    "        except Exception as e:\n",
    "            # Handle the exception, you can print the error message or handle it as per your requirement\n",
    "            print(f\"Error occurred while processing the URL: {url}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return positive_count, negative_count\n",
    "\n",
    "def search_google_news(query):\n",
    "    search_query = query + \"+Company\"\n",
    "    search_url = f\"https://www.google.com/search?q={search_query}&tbm=nws\"\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    search_results = soup.find_all('a', href=True)\n",
    "    result_urls = []\n",
    "\n",
    "    for result in search_results:\n",
    "        url = result['href']\n",
    "        if url.startswith('/url?q='):\n",
    "            url = url[7:]  # Remove '/url?q=' prefix\n",
    "            url = re.split('&', url, maxsplit=1)[0]  # Remove additional parameters\n",
    "            result_urls.append(url)\n",
    "\n",
    "    return result_urls\n",
    "\n",
    "# Example usage\n",
    "search_input = input(\"Enter a search query: \")\n",
    "news_urls = search_google_news(search_input)\n",
    "\n",
    "positive_count, negative_count = analyze_sentiment(news_urls)\n",
    "\n",
    "print(f\"Positive Articles Count: {positive_count}\")\n",
    "print(f\"Negative Articles Count: {negative_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "940318f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m             result_urls\u001b[38;5;241m.\u001b[39mappend(url)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_urls\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults_urls\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results_urls' is not defined"
     ]
    }
   ],
   "source": [
    "def search_google_news(query):\n",
    "    search_query = query + \"+Company\"\n",
    "    search_url = f\"https://www.google.com/search?q={search_query}&tbm=nws\"\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    search_results = soup.find_all('a', href=True)\n",
    "    result_urls = []\n",
    "\n",
    "    for result in search_results:\n",
    "        url = result['href']\n",
    "        if url.startswith('/url?q='):\n",
    "            url = url[7:]  # Remove '/url?q=' prefix\n",
    "            url = re.split('&', url, maxsplit=1)[0]  # Remove additional parameters\n",
    "            result_urls.append(url)\n",
    "\n",
    "    return result_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9046fdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while processing the URL: https://www.thestreet.com/cryptocurrency/apple-issues-warning-to-company-to-remove-bitcoin-payments-feature\n",
      "Error message: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.thestreet.com/cryptocurrency/apple-issues-warning-to-company-to-remove-bitcoin-payments-feature on URL https://www.thestreet.com/cryptocurrency/apple-issues-warning-to-company-to-remove-bitcoin-payments-feature\n",
      "Positive Articles Count: 3\n",
      "Negative Articles Count: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 80>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_urls\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m search_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter a search query: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m news_urls \u001b[38;5;241m=\u001b[39m search_google_news(search_input)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Print the URLs of the search results\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m     )\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyze_sentiment(urls):\n",
    "    # Initialize the sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Download and parse the article\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            # Get the article text content\n",
    "            article_text = article.text\n",
    "\n",
    "            # Analyze the sentiment of the article text\n",
    "            sentiment_scores = sid.polarity_scores(article_text)\n",
    "            compound_score = sentiment_scores['compound']\n",
    "\n",
    "            # Determine the sentiment label based on the compound score\n",
    "            sentiment_label = 'positive' if compound_score >= 0 else 'negative'\n",
    "\n",
    "            # Increment the corresponding sentiment count\n",
    "            if sentiment_label == 'positive':\n",
    "                positive_count += 1\n",
    "            else:\n",
    "                negative_count += 1\n",
    "        except Exception as e:\n",
    "            # Handle the exception, you can print the error message or handle it as per your requirement\n",
    "            print(f\"Error occurred while processing the URL: {url}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return positive_count, negative_count\n",
    "\n",
    "# Example usage\n",
    "article_urls = [\n",
    "    'https://www.bloomberg.com/news/articles/2023-06-12/apple-closes-at-record-in-latest-sign-of-big-tech-s-dominance#xj4y7vzkg',\n",
    "    'https://www.marketwatch.com/data-news/apple-inc-stock-rises-wednesday-still-underperforms-market-1b65a190-3ba63c6a2812-5f0339f1785f?mod=newsviewer_click&tesla=y',\n",
    "    'https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=newssearch&cd=&cad=rja&uact=8&ved=2ahUKEwjj2KfnrMX_AhXAg4kEHS1vAgwQxfQBKAB6BAgMEAE&url=https%3A%2F%2Fwww.theverge.com%2F2023%2F6%2F6%2F23751350%2Fapple-mira-ar-headset-startup&usg=AOvVaw2IrxWPeyTXLKOT0hkLtbvr',\n",
    "    'https://www.thestreet.com/cryptocurrency/apple-issues-warning-to-company-to-remove-bitcoin-payments-feature'\n",
    "]\n",
    "\n",
    "positive_count, negative_count = analyze_sentiment(article_urls)\n",
    "\n",
    "print(f\"Positive Articles Count: {positive_count}\")\n",
    "print(f\"Negative Articles Count: {negative_count}\")\n",
    "\n",
    "\n",
    "\n",
    "def search_google_news(query):\n",
    "    search_query = query + \"+Company\"\n",
    "    search_url = f\"https://www.google.com/search?q={search_query}&tbm=nws\"\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    search_results = soup.find_all('a', href=True)\n",
    "    result_urls = []\n",
    "\n",
    "    for result in search_results:\n",
    "        url = result['href']\n",
    "        if url.startswith('/url?q='):\n",
    "            url = url[7:]  # Remove '/url?q=' prefix\n",
    "            url = re.split('&', url, maxsplit=1)[0]  # Remove additional parameters\n",
    "            result_urls.append(url)\n",
    "\n",
    "    return result_urls\n",
    "\n",
    "# Example usage\n",
    "search_input = input(\"Enter a search query: \")\n",
    "news_urls = search_google_news(search_input)\n",
    "\n",
    "# Print the URLs of the search results\n",
    "for url in news_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d48855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def search_google_news(query):\n",
    "    search_query = query + \" Company\"\n",
    "    search_url = f\"https://www.google.com/search?q={search_query}&tbm=nws\"\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    search_results = soup.find_all('a', href=True)\n",
    "    result_urls = []\n",
    "\n",
    "    for result in search_results:\n",
    "        url = result['href']\n",
    "        if url.startswith('/url?q='):\n",
    "            url = url[7:]  # Remove '/url?q=' prefix\n",
    "            url = re.split('&', url, maxsplit=1)[0]  # Remove additional parameters\n",
    "            result_urls.append(url)\n",
    "\n",
    "    return result_urls\n",
    "\n",
    "# Example usage\n",
    "search_input = input(\"Enter a search query: \")\n",
    "news_urls = search_google_news(search_input)\n",
    "\n",
    "# Print the URLs of the search results\n",
    "for url in news_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0cf838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search query: apple\n",
      "https://www.bloomberg.com/news/articles/2023-06-12/apple-closes-at-record-in-latest-sign-of-big-tech-s-dominance\n",
      "https://www.thestreet.com/cryptocurrency/apple-issues-warning-to-company-to-remove-bitcoin-payments-feature\n",
      "https://www.marketwatch.com/story/apple-inc-stock-rises-wednesday-still-underperforms-market-1b65a190-3ba63c6a2812%3Fmod%3Dnewsviewer_click\n",
      "https://www.theverge.com/2023/6/6/23751350/apple-mira-ar-headset-startup\n",
      "https://www.barrons.com/articles/apple-iphone-15-upgrades-stock-a187c9e6\n",
      "https://www.reuters.com/legal/apple-amazon-must-face-consumer-lawsuit-over-iphone-ipad-prices-us-judge-2023-06-09/\n",
      "https://finance.yahoo.com/news/apples-lesser-known-co-founder-180031182.html\n",
      "https://www.ft.com/content/794292d8-759f-491a-9776-98fb687c45cc\n",
      "https://www.usatoday.com/story/tech/2023/06/05/wwdc-2023-apple-keynote-live-updates/70287849007/\n",
      "https://www.theregister.com/2023/05/19/apple_chatgpt/\n",
      "https://support.google.com/websearch%3Fp%3Dws_settings_location%26hl%3Den\n",
      "https://accounts.google.com/ServiceLogin%3Fcontinue%3Dhttps://www.google.com/search%253Fq%253Dapple%252BCompany%2526tbm%253Dnws%26hl%3Den\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def search_google_news(query):\n",
    "    search_query = query + \" Company\"\n",
    "    search_url = f\"https://www.google.com/search?q={search_query}&tbm=nws\"\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    search_results = soup.find_all('a', href=True)\n",
    "    article_urls = []\n",
    "\n",
    "    for result in search_results:\n",
    "        url = result['href']\n",
    "        if url.startswith('/url?q='):\n",
    "            url = url[7:]  # Remove '/url?q=' prefix\n",
    "            url = re.split('&', url, maxsplit=1)[0]  # Remove additional parameters\n",
    "            article_urls.append(url)\n",
    "\n",
    "    return article_urls\n",
    "\n",
    "# Example usage\n",
    "search_input = input(\"Enter a search query: \")\n",
    "article_urls = search_google_news(search_input)\n",
    "\n",
    "# Print the URLs of the search results\n",
    "for url in article_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a4dbccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search query: lovesac\n",
      "Error occurred while processing the URL: https://accounts.google.com/ServiceLogin%3Fcontinue%3Dhttps://www.google.com/search%253Fq%253Dlovesac%252BCompany%2526tbm%253Dnws%26hl%3Den\n",
      "Error message: Article `download()` failed with 404 Client Error: Not Found for url: https://accounts.google.com/ServiceLogin%3Fcontinue%3Dhttps://www.google.com/search%253Fq%253Dlovesac%252BCompany%2526tbm%253Dnws%26hl%3Den on URL https://accounts.google.com/ServiceLogin%3Fcontinue%3Dhttps://www.google.com/search%253Fq%253Dlovesac%252BCompany%2526tbm%253Dnws%26hl%3Den\n",
      "Positive Articles Count: 10\n",
      "Negative Articles Count: 1\n"
     ]
    }
   ],
   "source": [
    "#includes errors and error messages\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyze_sentiment(urls):\n",
    "    # Initialize the sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Download and parse the article\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            # Get the article text content\n",
    "            article_text = article.text\n",
    "\n",
    "            # Analyze the sentiment of the article text\n",
    "            sentiment_scores = sid.polarity_scores(article_text)\n",
    "            compound_score = sentiment_scores['compound']\n",
    "\n",
    "            # Determine the sentiment label based on the compound score\n",
    "            sentiment_label = 'positive' if compound_score >= 0 else 'negative'\n",
    "\n",
    "            # Increment the corresponding sentiment count\n",
    "            if sentiment_label == 'positive':\n",
    "                positive_count += 1\n",
    "            else:\n",
    "                negative_count += 1\n",
    "        except Exception as e:\n",
    "            # Handle the exception, you can print the error message or handle it as per your requirement\n",
    "            print(f\"Error occurred while processing the URL: {url}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return positive_count, negative_count\n",
    "\n",
    "# Example usage\n",
    "def search_google_news(query):\n",
    "    search_query = query + \" Company\"\n",
    "    search_url = f\"https://www.google.com/search?q={search_query}&tbm=nws\"\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    search_results = soup.find_all('a', href=True)\n",
    "    article_urls = []\n",
    "\n",
    "    for result in search_results:\n",
    "        url = result['href']\n",
    "        if url.startswith('/url?q='):\n",
    "            url = url[7:]  # Remove '/url?q=' prefix\n",
    "            url = re.split('&', url, maxsplit=1)[0]  # Remove additional parameters\n",
    "            article_urls.append(url)\n",
    "\n",
    "    return article_urls\n",
    "\n",
    "# Example usage\n",
    "search_input = input(\"Enter a search query: \")\n",
    "article_urls = search_google_news(search_input)\n",
    "\n",
    "positive_count, negative_count = analyze_sentiment(article_urls)\n",
    "\n",
    "print(f\"Positive Articles Count: {positive_count}\")\n",
    "print(f\"Negative Articles Count: {negative_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3bb8cf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Company: microsoft\n",
      "Total Articles in search 12\n",
      "Total Articles Analyzed 9\n",
      "Positive Articles Count: 7\n",
      "Negative Articles Count: 2\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyze_sentiment(urls):\n",
    "    # Initialize the sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Download and parse the article\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            # Get the article text content\n",
    "            article_text = article.text\n",
    "\n",
    "            # Analyze the sentiment of the article text\n",
    "            sentiment_scores = sid.polarity_scores(article_text)\n",
    "            compound_score = sentiment_scores['compound']\n",
    "\n",
    "            # Determine the sentiment label based on the compound score\n",
    "            sentiment_label = 'positive' if compound_score >= 0 else 'negative'\n",
    "\n",
    "            # Increment the corresponding sentiment count\n",
    "            if sentiment_label == 'positive':\n",
    "                positive_count += 1\n",
    "            else:\n",
    "                negative_count += 1\n",
    "        except Exception as e:\n",
    "            # Handle the exception, you can print the error message or handle it as per your requirement\n",
    "            continue\n",
    "\n",
    "    return positive_count, negative_count\n",
    "\n",
    "# Example usage\n",
    "def search_google_news(query):\n",
    "    search_query = query + \" Company\"\n",
    "    search_url = f\"https://www.google.com/search?q={search_query}&tbm=nws\"\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    search_results = soup.find_all('a', href=True)\n",
    "    article_urls = []\n",
    "\n",
    "    for result in search_results:\n",
    "        url = result['href']\n",
    "        if url.startswith('/url?q='):\n",
    "            url = url[7:]  # Remove '/url?q=' prefix\n",
    "            url = re.split('&', url, maxsplit=1)[0]  # Remove additional parameters\n",
    "            article_urls.append(url)\n",
    "\n",
    "    return article_urls\n",
    "\n",
    "# Example usage\n",
    "search_input = input(\"Enter Company: \")\n",
    "article_urls = search_google_news(search_input)\n",
    "\n",
    "positive_count, negative_count = analyze_sentiment(article_urls)\n",
    "\n",
    "Size = len(article_urls)\n",
    "Analyzed_articles = positive_count+negative_count\n",
    "print(\"Total Articles in search \" + str(Size))\n",
    "print(\"Total Articles Analyzed \"+ str(Analyzed_articles)) \n",
    "print(f\"Positive Articles Count: {positive_count}\")\n",
    "print(f\"Negative Articles Count: {negative_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f2aba41d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2366666683.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [50]\u001b[1;36m\u001b[0m\n\u001b[1;33m    git add\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a2a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
